{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Space Report\n",
    "\n",
    "\n",
    "<img src=\"images/polito_logo.png\" alt=\"Polito Logo\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "## Pittsburgh Bridges Data Set\n",
    "\n",
    "<img src=\"images/andy_warhol_bridge.jpg\" alt=\"Andy Warhol Bridge\" style=\"width: 200px;\"/>\n",
    "\n",
    "    Andy Warhol Bridge - Pittsburgh.\n",
    "\n",
    "Report created by Student Francesco Maria Chiarlo s253666, for A.A 2019/2020.\n",
    "\n",
    "**Abstract**:The aim of this report is to evaluate the effectiveness of distinct, different statistical learning approaches, in particular focusing on their characteristics as well as on their advantages and backwards when applied onto a relatively small dataset as the one employed within this report, that is Pittsburgh Bridgesdataset.\n",
    "\n",
    "**Key words**:Statistical Learning, Machine Learning, Bridge Design.\n",
    "\n",
    "## TOC:\n",
    "* [Imports Section](#imports-section)\n",
    "* [Dataset's Attributes Description](#attributes-description)\n",
    "* [Data Preparation and Investigation](#data-preparation)\n",
    "* [Learning Models](#learning-models)\n",
    "* [Improvements and Conclusions](#improvements-and-conclusions)\n",
    "* [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports Section <a class=\"anchor\" id=\"imports-section\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================== #\n",
    "# STANDARD IMPORTS\n",
    "# =========================================================================== #\n",
    "print(__doc__)\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "# Matplotlib pyplot provides plotting API\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import chart_studio.plotly.plotly as py\n",
    "import seaborn as sns;  sns.set(style=\"ticks\", color_codes=True) # sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================== #\n",
    "# UTILS IMPORTS (Done by myself)\n",
    "# =========================================================================== #\n",
    "from utils.display_utils import display_heatmap\n",
    "from utils.display_utils import show_frequency_distribution_predictors\n",
    "from utils.display_utils import show_frequency_distribution_predictor\n",
    "from utils.display_utils import build_boxplot\n",
    "from utils.display_utils import show_categorical_predictor_values\n",
    "from utils.display_utils import  show_cum_variance_vs_components\n",
    "from utils.display_utils import show_histograms_from_heatmap_corr_matrix\n",
    "from utils.display_utils import show_pca_1_vs_pca_2_pcaKernel\n",
    "from utils.display_utils import show_scatter_plots_pcaKernel\n",
    "from utils.display_utils import show_overall_dataset_scatter_plots\n",
    "\n",
    "from utils.preprocessing_utils import preprocess_categorical_variables\n",
    "from utils.preprocessing_utils import  preprocessing_data_rescaling\n",
    "\n",
    "from utils.training_utils import sgd_classifier_grid_search\n",
    "from utils.training_utils import naive_bayes_classifier_grid_search\n",
    "from utils.training_utils import svm_linear_classifier_grid_search\n",
    "from utils.training_utils import decision_tree_classifier_grid_search\n",
    "from utils.training_utils import random_forest_classifier_grid_search\n",
    "from utils.training_utils import plot_roc_crossval\n",
    "\n",
    "from utils.training_utils_v2 import fit_by_n_components, fit_all_by_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================== #\n",
    "# sklearn IMPORT\n",
    "# =========================================================================== #\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "# Import scikit-learn classes: models (Estimators).\n",
    "from sklearn.naive_bayes import GaussianNB           # Non-parametric Generative Model\n",
    "from sklearn.naive_bayes import MultinomialNB        # Non-parametric Generative Model\n",
    "from sklearn.linear_model import LinearRegression    # Parametric Linear Discriminative Model\n",
    "from sklearn.linear_model import LogisticRegression  # Parametric Linear Discriminative Model\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC                          # Parametric Linear Discriminative \"Support Vector Classifier\"\n",
    "from sklearn.tree import DecisionTreeClassifier      # Non-parametric Model\n",
    "from sklearn.ensemble import BaggingClassifier       # Non-parametric Model (Meta-Estimator, that is, an Ensemble Method)\n",
    "from sklearn.ensemble import RandomForestClassifier  # Non-parametric Model (Meta-Estimator, that is, an Ensemble Method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset's Attributes Description <a class=\"anchor\" id=\"attributes-description\"></a>\n",
    "\n",
    "The analyses that I aim at accomplishing while using as means the methods or approaches provided by both Statistical Learning and Machine Learning fields, concern the dataset Pittsburgh Bridges, and what follows is a overview and brief description of the main characteristics, as well as, basic information about this precise dataset.\n",
    "\n",
    "The Pittsburgh Bridges dataset is a dataset available from the web site called mainly *\"UCI Machine Learing Repository\"*, which is one of the well known web site that let a large amount of different datasets, from different domains or fields, to be used for machine-learning research and which have been cited in peer-reviewed academic journals.\n",
    "\n",
    "In particular, the dataset I'm going to treat and analyze, which is Pittsburgh Bridges dataset, has been made freely available from the Western Pennsylvania Regional Data Center (WPRDC), which is a project led by the University Center of Social and Urban Research (UCSUR) at the University of Pittsburgh (\"University\") in collaboration with City of Pittsburgh and The County of Allegheny in Pennsylvania. The WPRDC and the WPRDC Project is supported by a grant from the Richard King Mellon Foundation.\n",
    "\n",
    "In order to be more precise, from the official and dedicated web page, within UCI Machine Learning cite, Pittsburgh Bridges dataset is a dataset that has been created after the works of some co-authors which are:\n",
    "- Yoram Reich & Steven J. Fenves from Department of Civil Engineering and Engineering Design Research Center Carnegie Mellon University Pittsburgh, PA 15213\n",
    "\n",
    "The Pittsburgh Bridges dataset is made of up to 108 distinct observations and each of that data sample is made of 12 attributes or features where some of them are considered to be continuous properties and other to be categorical or nominal properties. Those variables are the following:\n",
    "\n",
    "- **RIVER**: which is a nominal type variable that can assume the subsequent possible discrete values which are: A, M, O. Where A stands for Allegheny river, while M stands for Monongahela river and lastly O stands for Ohio river.\n",
    "- **LOCATION**: which represents a nominal type variable too, and assume a positive integer value from 1 up to 52 used as categorical attribute.\n",
    "- **ERECTED**: which might be either a numerical or categorical variable, depending on the fact that we want to aggregate a bunch of value under a categorical quantity. What this means is that, basically such attribute is made of date starting from 1818 up to 1986, but we may imagine to aggregate somehow these data within a given category among those suggested, that are CRAFTS, EMERGENING, MATURE, MODERN.\n",
    "- **PURPOSE**: which is a categorical attribute and represents the reason why a particular bridge has been built, which means that this attribute represents what kind of vehicle can cross the bridge or if the bridge has been made just for people. For this reasons the allowd values for this attributes are the following: WALK, AQUEDUCT, RR, HIGHWAY. Three out of four are self explained values, while RR value that might be tricky at first glance, it just stands for railroad.\n",
    "- **LENGTH**: which represents the bridge's length, is a numerical attribute if we just look at the real number values that go from 804 up to 4558, but we can again decide to handle or arrange such values so that they can be grouped into range of values mapped into SHORT, MEDIUM, LONG so that we can refer to a bridge's length by means of these new categorical values.\n",
    "- **LANES**: which is a categorical variable which is represented by numerical values, that are 1, 2, 4, 6 which indicate the number of distinct lanes that a bridge in Pittsburgh city may have. The larger the value the wider the bridge.\n",
    "- **CLEAR-G**: specifies  whether  a  vertical  navigation clearance requirement was enforced in the design or not.\n",
    "- **T-OR-D**: which is a nominal attribute, in other words, a categorical attribute that can assume THROUGH, DECK values. In order to be more precise, this samples attribute deals with structural elements of a bridge. In fact, a deck is the surface of a bridge and this structural element, of bridge's superstructure, may be constructed of concrete, steel, open grating, or wood. On the other hand, a through arch bridge, also known as a half-through arch bridge or a through-type arch bridge, is a bridge that is made from materials such as steel or reinforced concrete, in which the base of an arch structure is below the deck but the top rises above it.\n",
    "- **MATERIAL**: which is a categorical or nominal variable and is used to describe the bridge telling which is the main or core material used to build it.\n",
    "  This attribute can assume one of the possible, following values which are: WOOD, IRON, STEEL. Furthermore, we expect to see somehow a bit of correlation between the values assumed by the pairs represented by T-OR-D and MATERIAL columns, when looking just to them.\n",
    "- **SPAN**: which is a categorical or nominal value and has been recorded by means of three possible values for each sample, that are SHORT, MEDIUM, LONG. This attribute, within the field of Structural Engineering, is the distance between two intermediate supports for a structure, e.g. a beam or a bridge. A span can be closed by a solid beam or by a rope. The first kind is used for bridges, the second one for power lines, overhead telecommunication lines, some type of antennas or for aerial tramways. \n",
    "- **REL-L**: which is a categorical or nominal variable and stands for relative length of the main span of the bridge to the total crossing length, it can assume three possible values that are S, S-F, F.\n",
    "- Lastly, **TYPE** which indicates as a categorical or nominal attributes what type of bridge each record represents, among the possible 6 distinct classes or types of bridges that are: WOOD, SUSPEN, SIMPLE-T, ARCH, CANTILEV, CONT-T."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation and Investigation <a class=\"anchor\" id=\"data-preparation\"></a>\n",
    "\n",
    "The aim of this chapter is to get in the data, that are available within Pittsburgh Bridge Dataset, in order to investigate a bit more in to detail and generally speaking deeper the main or high level statistics quantities, such as mean, median, standard deviation of each attribute, as well as displaying somehow data distribution for each attribute by means of histogram plots. This phase allows or enables us to decide which should be the best feature to be selected as the target variable, in other word the attribute that will represent the dependent variable with respect to the remaining attributes that instead will play the role of predictors and independent variables, as well.\n",
    "\n",
    "In order to investigate and explore our data we make usage of *Pandas library*. We recall mainly that, in computer programming, Pandas is a software library written for the Python programming language* for *data manipulation and analysis*. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software and a interesting and funny things about such tool is that the name is derived from the term \"panel data\", an econometrics term for data sets that include observations over multiple time periods for the same individuals.\n",
    "We also note that as the analysis proceeds we will introduce other computer programming as well as programming libraries that allow or enable us to fulfill our goals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, once I have downloaded from the provided web page the dataset with the data samples about Pittsburgh Bridge we load the data by means of functions available using python library's pandas. We notice that the overall set of data points is large up to 108 records or rows, which are sorted by Erected attributes, so this means that are sorted in decreasing order from the oldest bridge which has been built in 1818 up to the most modern bridge that has been erected in 1986. Then we display the first 5 rows to get an overview and have a first idea about what is inside the overall dataset, and the result we obtain by means of head() function applied onto the fetched dataset is equals to what follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================== #\n",
    "# READ INPUT DATASET\n",
    "# =========================================================================== #\n",
    "\n",
    "dataset_path = 'C:\\\\Users\\\\Francesco\\Documents\\\\datasets\\\\pittsburgh_dataset'\n",
    "dataset_name = 'bridges.data.csv'\n",
    "\n",
    "TARGET_COL = 'T-OR-D'  \n",
    "\n",
    "# column_names = ['IDENTIF', 'RIVER', 'LOCATION', 'ERECTED', 'PURPOSE', 'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL', 'SPAN', 'REL-L', 'TYPE']\n",
    "column_names = ['RIVER', 'LOCATION', 'ERECTED', 'PURPOSE', 'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL', 'SPAN', 'REL-L', 'TYPE']\n",
    "dataset = pd.read_csv(os.path.join(dataset_path, dataset_name), names=column_names, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (108, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 108 entries, E1 to E109\n",
      "Data columns (total 12 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   RIVER     108 non-null    object\n",
      " 1   LOCATION  108 non-null    object\n",
      " 2   ERECTED   108 non-null    int64 \n",
      " 3   PURPOSE   108 non-null    object\n",
      " 4   LENGTH    108 non-null    object\n",
      " 5   LANES     108 non-null    object\n",
      " 6   CLEAR-G   108 non-null    object\n",
      " 7   T-OR-D    108 non-null    object\n",
      " 8   MATERIAL  108 non-null    object\n",
      " 9   SPAN      108 non-null    object\n",
      " 10  REL-L     108 non-null    object\n",
      " 11  TYPE      108 non-null    object\n",
      "dtypes: int64(1), object(11)\n",
      "memory usage: 11.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# SHOW SOME STANDARD DATASET INFOS\n",
    "# --------------------------------------------------------------------------- #\n",
    "print('Dataset shape: {}'.format(dataset.shape))\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RIVER</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ERECTED</th>\n",
       "      <th>PURPOSE</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>LANES</th>\n",
       "      <th>CLEAR-G</th>\n",
       "      <th>T-OR-D</th>\n",
       "      <th>MATERIAL</th>\n",
       "      <th>SPAN</th>\n",
       "      <th>REL-L</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E1</th>\n",
       "      <td>M</td>\n",
       "      <td>3</td>\n",
       "      <td>1818</td>\n",
       "      <td>HIGHWAY</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>THROUGH</td>\n",
       "      <td>WOOD</td>\n",
       "      <td>SHORT</td>\n",
       "      <td>S</td>\n",
       "      <td>WOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E2</th>\n",
       "      <td>A</td>\n",
       "      <td>25</td>\n",
       "      <td>1819</td>\n",
       "      <td>HIGHWAY</td>\n",
       "      <td>1037</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>THROUGH</td>\n",
       "      <td>WOOD</td>\n",
       "      <td>SHORT</td>\n",
       "      <td>S</td>\n",
       "      <td>WOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E3</th>\n",
       "      <td>A</td>\n",
       "      <td>39</td>\n",
       "      <td>1829</td>\n",
       "      <td>AQUEDUCT</td>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>THROUGH</td>\n",
       "      <td>WOOD</td>\n",
       "      <td>?</td>\n",
       "      <td>S</td>\n",
       "      <td>WOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E5</th>\n",
       "      <td>A</td>\n",
       "      <td>29</td>\n",
       "      <td>1837</td>\n",
       "      <td>HIGHWAY</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>THROUGH</td>\n",
       "      <td>WOOD</td>\n",
       "      <td>SHORT</td>\n",
       "      <td>S</td>\n",
       "      <td>WOOD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E6</th>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>1838</td>\n",
       "      <td>HIGHWAY</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>N</td>\n",
       "      <td>THROUGH</td>\n",
       "      <td>WOOD</td>\n",
       "      <td>?</td>\n",
       "      <td>S</td>\n",
       "      <td>WOOD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RIVER LOCATION  ERECTED   PURPOSE LENGTH LANES CLEAR-G   T-OR-D MATERIAL  \\\n",
       "E1     M        3     1818   HIGHWAY      ?     2       N  THROUGH     WOOD   \n",
       "E2     A       25     1819   HIGHWAY   1037     2       N  THROUGH     WOOD   \n",
       "E3     A       39     1829  AQUEDUCT      ?     1       N  THROUGH     WOOD   \n",
       "E5     A       29     1837   HIGHWAY   1000     2       N  THROUGH     WOOD   \n",
       "E6     M       23     1838   HIGHWAY      ?     2       N  THROUGH     WOOD   \n",
       "\n",
       "     SPAN REL-L  TYPE  \n",
       "E1  SHORT     S  WOOD  \n",
       "E2  SHORT     S  WOOD  \n",
       "E3      ?     S  WOOD  \n",
       "E5  SHORT     S  WOOD  \n",
       "E6      ?     S  WOOD  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SHOWING FIRSTS N-ROWS AS THEY ARE STORED WITHIN DATASET\n",
    "# --------------------------------------------------------------------------- #\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can notice from just the table above is that there are some attributes that are characterized by a special character that is '?' which stands for a missing value, so by chance there was not possibility to get the value for this attribute, such as for LENGTH and SPAN attributes. Analyzing in more details the dataset we discover that there are up to 6 different attributes, in the majority attributes with categorical or nominal nature such as CLEAR-G, T-OR-D, MATERIAL, SPAN, REL-L, and TYPE that contain at list one row characterized by the fact that one of its attributes is set to assuming '?' value that stands, as we already know for a missing value.\n",
    "\n",
    "Here, we can follow different strategies that depends onto the level of complexity as well as accuracy we want to obtain or achieve for models we are going to fit to the data after having correctly pre-processed them, speaking about what we could do with missing values. In fact one can follow the simplest way and can decide to simply discard those rows that contain at least one attribute with a missing value represented by the '?' symbol. Otherwise one may alos decide to follow a different strategy that aims at keeping also those rows that have some missing values by means of some kind of technique that allows to establish a potential substituting value for the missing one.\n",
    "\n",
    "So, in this setting, that is our analyses, we start by just leaving out those rows that at least contain one attribute that has a missing value, this choice leads us to reduce the size of our dataset from 108 records to 70 remaining samples, with a drop of 38 data examples, which may affect the final results, since we left out more or less the 46\\% of the data because of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before preprocessing dataset and handling null values\n",
      "There are any null values ? Response: False\n",
      "Number of null values for each predictor:\n",
      "RIVER       0\n",
      "LOCATION    0\n",
      "ERECTED     0\n",
      "PURPOSE     0\n",
      "LENGTH      0\n",
      "LANES       0\n",
      "CLEAR-G     0\n",
      "T-OR-D      0\n",
      "MATERIAL    0\n",
      "SPAN        0\n",
      "REL-L       0\n",
      "TYPE        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# INVESTIGATING DATASET IN ORDER TO DETECT NULL VALUES\n",
    "# --------------------------------------------------------------------------- #\n",
    "print('Before preprocessing dataset and handling null values')\n",
    "result = dataset.isnull().values.any()\n",
    "print('There are any null values ? Response: {}'.format(result))\n",
    "\n",
    "result = dataset.isnull().sum()\n",
    "print('Number of null values for each predictor:\\n{}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIVER    : ['A', 'M', 'O', 'Y']\n",
      "PURPOSE  : ['AQUEDUCT', 'HIGHWAY', 'RR', 'WALK']\n",
      "CLEAR-G  : ['?', 'G', 'N']\n",
      "T-OR-D   : ['?', 'DECK', 'THROUGH']\n",
      "MATERIAL : ['?', 'IRON', 'STEEL', 'WOOD']\n",
      "SPAN     : ['?', 'LONG', 'MEDIUM', 'SHORT']\n",
      "REL-L    : ['?', 'F', 'S', 'S-F']\n",
      "TYPE     : ['?', 'ARCH', 'CANTILEV', 'CONT-T', 'NIL', 'SIMPLE-T', 'SUSPEN', 'WOOD']\n"
     ]
    }
   ],
   "source": [
    "# DISCOVERING VALUES WITHIN EACH PREDICTOR DOMAIN\n",
    "# --------------------------------------------------------------------------- #\n",
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION', 'LANES']\n",
    "# columns_2_avoid = None\n",
    "list_columns_2_fix = show_categorical_predictor_values(dataset, columns_2_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Before\" removing '?' rows, Dataset dim: (108, 12)\n",
      "\"After\" removing '?' rows, Dataset dim:  (88, 12)\n",
      "--------------------------------------------------\n",
      "RIVER    : ['A', 'M', 'O', 'Y']\n",
      "PURPOSE  : ['AQUEDUCT', 'HIGHWAY', 'RR']\n",
      "CLEAR-G  : ['G', 'N']\n",
      "T-OR-D   : ['DECK', 'THROUGH']\n",
      "MATERIAL : ['IRON', 'STEEL', 'WOOD']\n",
      "SPAN     : ['LONG', 'MEDIUM', 'SHORT']\n",
      "REL-L    : ['F', 'S', 'S-F']\n",
      "TYPE     : ['ARCH', 'CANTILEV', 'CONT-T', 'SIMPLE-T', 'SUSPEN', 'WOOD']\n"
     ]
    }
   ],
   "source": [
    "# FIXING, UPDATING NULL VALUES CODED AS '?' SYMBOL\n",
    "# WITHIN EACH CATEGORICAL VARIABLE, IF DETECTED ANY\n",
    "# --------------------------------------------------------------------------- #\n",
    "print('\"Before\" removing \\'?\\' rows, Dataset dim:', dataset.shape)\n",
    "for _, predictor in enumerate(list_columns_2_fix):\n",
    "    dataset = dataset[dataset[predictor] != '?']\n",
    "print('\"After\" removing \\'?\\' rows, Dataset dim: ', dataset.shape)\n",
    "print('-' * 50)\n",
    "\n",
    "_ = show_categorical_predictor_values(dataset, columns_2_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 88 entries, E1 to E90\n",
      "Data columns (total 12 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   RIVER     88 non-null     int64 \n",
      " 1   LOCATION  88 non-null     object\n",
      " 2   ERECTED   88 non-null     int64 \n",
      " 3   PURPOSE   88 non-null     int64 \n",
      " 4   LENGTH    88 non-null     object\n",
      " 5   LANES     88 non-null     object\n",
      " 6   CLEAR-G   88 non-null     int64 \n",
      " 7   T-OR-D    88 non-null     int64 \n",
      " 8   MATERIAL  88 non-null     int64 \n",
      " 9   SPAN      88 non-null     int64 \n",
      " 10  REL-L     88 non-null     int64 \n",
      " 11  TYPE      88 non-null     int64 \n",
      "dtypes: int64(9), object(3)\n",
      "memory usage: 8.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# INTERMEDIATE RESULT FOUND\n",
    "# --------------------------------------------------------------------------- #\n",
    "preprocess_categorical_variables(dataset, columns_2_avoid)\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RIVER</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ERECTED</th>\n",
       "      <th>PURPOSE</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>LANES</th>\n",
       "      <th>CLEAR-G</th>\n",
       "      <th>T-OR-D</th>\n",
       "      <th>MATERIAL</th>\n",
       "      <th>SPAN</th>\n",
       "      <th>REL-L</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E1</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1818</td>\n",
       "      <td>2</td>\n",
       "      <td>?</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E2</th>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1819</td>\n",
       "      <td>2</td>\n",
       "      <td>1037</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E5</th>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1837</td>\n",
       "      <td>2</td>\n",
       "      <td>1000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E7</th>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1840</td>\n",
       "      <td>2</td>\n",
       "      <td>990</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E8</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>1844</td>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RIVER LOCATION  ERECTED  PURPOSE LENGTH LANES  CLEAR-G  T-OR-D  MATERIAL  \\\n",
       "E1      2        3     1818        2      ?     2        2       2         3   \n",
       "E2      1       25     1819        2   1037     2        2       2         3   \n",
       "E5      1       29     1837        2   1000     2        2       2         3   \n",
       "E7      1       27     1840        2    990     2        2       2         3   \n",
       "E8      1       28     1844        1   1000     1        2       2         1   \n",
       "\n",
       "    SPAN  REL-L  TYPE  \n",
       "E1     3      2     6  \n",
       "E2     3      2     6  \n",
       "E5     3      2     6  \n",
       "E7     2      2     6  \n",
       "E8     3      2     5  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is represented by the effort of mapping categorical variables into numerical variables, so that them are comparable with the already existing numerical or continuous variables, and also by mapping the categorical variables into numerical variables we allow or enable us to perform some kind of normalization or just transformation onto the entire dataset in order to let some machine learning algorithm to work better or to take advantage of normalized data within our pre-processed dataset. Furthermore, by transforming first the categorical attributes into a continuous version we are also able to calculate the \\textit{heatmap}, which is a very useful way of representing a correlation matrix calculated on the whole dataset. Moreover we have displayed data distribution for each attribute by means of histogram representation to take some useful information about the number of occurrences for each possible value, in particular for those attributes that have a categorical nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before (88, 12)\n",
      "After (80, 12)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 80 entries, E1 to E90\n",
      "Data columns (total 12 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   RIVER     80 non-null     int64 \n",
      " 1   LOCATION  80 non-null     object\n",
      " 2   ERECTED   80 non-null     int32 \n",
      " 3   PURPOSE   80 non-null     int64 \n",
      " 4   LENGTH    80 non-null     object\n",
      " 5   LANES     80 non-null     int32 \n",
      " 6   CLEAR-G   80 non-null     int64 \n",
      " 7   T-OR-D    80 non-null     int64 \n",
      " 8   MATERIAL  80 non-null     int64 \n",
      " 9   SPAN      80 non-null     int64 \n",
      " 10  REL-L     80 non-null     int64 \n",
      " 11  TYPE      80 non-null     int64 \n",
      "dtypes: int32(2), int64(8), object(2)\n",
      "memory usage: 7.5+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# MAP NUMERICAL VALUES TO INTEGER VALUES\n",
    "# --------------------------------------------------------------------------- #\n",
    "print('Before', dataset.shape)\n",
    "columns_2_map = ['ERECTED', 'LANES']\n",
    "for _, predictor in enumerate(columns_2_map):\n",
    "    dataset = dataset[dataset[predictor] != '?']\n",
    "    dataset[predictor] = np.array(list(map(lambda x: int(x), dataset[predictor].values)))\n",
    "print('After', dataset.shape)\n",
    "print(dataset.info())\n",
    "# print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP NUMERICAL VALUES TO FLOAT VALUES\n",
    "# --------------------------------------------------------------------------- #\n",
    "# print('Before', dataset.shape)\n",
    "columns_2_map = ['LOCATION', 'LANES', 'LENGTH']    \n",
    "for _, predictor in enumerate(columns_2_map):\n",
    "    dataset = dataset[dataset[predictor] != '?']\n",
    "    dataset[predictor] = np.array(list(map(lambda x: float(x), dataset[predictor].values)))\n",
    "# print('After', dataset.shape)    \n",
    "# print(dataset.info()); # print(dataset.head(5))\n",
    "\n",
    "# columns_2_avoid = None\n",
    "# list_columns_2_fix = show_categorical_predictor_values(dataset, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RIVER</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ERECTED</th>\n",
       "      <th>PURPOSE</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>LANES</th>\n",
       "      <th>CLEAR-G</th>\n",
       "      <th>T-OR-D</th>\n",
       "      <th>MATERIAL</th>\n",
       "      <th>SPAN</th>\n",
       "      <th>REL-L</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E2</th>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1819</td>\n",
       "      <td>2</td>\n",
       "      <td>1037.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E5</th>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1837</td>\n",
       "      <td>2</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E7</th>\n",
       "      <td>1</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1840</td>\n",
       "      <td>2</td>\n",
       "      <td>990.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E8</th>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1844</td>\n",
       "      <td>1</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>E9</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1846</td>\n",
       "      <td>2</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    RIVER  LOCATION  ERECTED  PURPOSE  LENGTH  LANES  CLEAR-G  T-OR-D  \\\n",
       "E2      1      25.0     1819        2  1037.0    2.0        2       2   \n",
       "E5      1      29.0     1837        2  1000.0    2.0        2       2   \n",
       "E7      1      27.0     1840        2   990.0    2.0        2       2   \n",
       "E8      1      28.0     1844        1  1000.0    1.0        2       2   \n",
       "E9      2       3.0     1846        2  1500.0    2.0        2       2   \n",
       "\n",
       "    MATERIAL  SPAN  REL-L  TYPE  \n",
       "E2         3     3      2     6  \n",
       "E5         3     3      2     6  \n",
       "E7         3     2      2     6  \n",
       "E8         1     3      2     5  \n",
       "E9         1     3      2     5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = dataset.isnull().values.any()\n",
    "# print('After handling null values\\nThere are any null values ? Response: {}'.format(result))\n",
    "\n",
    "result = dataset.isnull().sum()\n",
    "# print('Number of null values for each predictor:\\n{}'.format(result))\n",
    "\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RIVER</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ERECTED</th>\n",
       "      <th>PURPOSE</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>LANES</th>\n",
       "      <th>CLEAR-G</th>\n",
       "      <th>T-OR-D</th>\n",
       "      <th>MATERIAL</th>\n",
       "      <th>SPAN</th>\n",
       "      <th>REL-L</th>\n",
       "      <th>TYPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.642857</td>\n",
       "      <td>25.438571</td>\n",
       "      <td>1911.542857</td>\n",
       "      <td>2.214286</td>\n",
       "      <td>1597.657143</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>1.257143</td>\n",
       "      <td>1.814286</td>\n",
       "      <td>2.071429</td>\n",
       "      <td>1.742857</td>\n",
       "      <td>1.728571</td>\n",
       "      <td>3.457143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.723031</td>\n",
       "      <td>13.223347</td>\n",
       "      <td>36.010339</td>\n",
       "      <td>0.478308</td>\n",
       "      <td>780.237680</td>\n",
       "      <td>1.242785</td>\n",
       "      <td>0.440215</td>\n",
       "      <td>0.391684</td>\n",
       "      <td>0.428054</td>\n",
       "      <td>0.629831</td>\n",
       "      <td>0.797122</td>\n",
       "      <td>1.575958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1819.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>840.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>15.250000</td>\n",
       "      <td>1891.250000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.500000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>1915.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1325.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>35.750000</td>\n",
       "      <td>1935.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.750000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1978.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4558.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           RIVER   LOCATION      ERECTED    PURPOSE       LENGTH      LANES  \\\n",
       "count  70.000000  70.000000    70.000000  70.000000    70.000000  70.000000   \n",
       "mean    1.642857  25.438571  1911.542857   2.214286  1597.657143   2.857143   \n",
       "std     0.723031  13.223347    36.010339   0.478308   780.237680   1.242785   \n",
       "min     1.000000   1.000000  1819.000000   1.000000   840.000000   1.000000   \n",
       "25%     1.000000  15.250000  1891.250000   2.000000  1000.000000   2.000000   \n",
       "50%     1.500000  27.000000  1915.000000   2.000000  1325.000000   2.000000   \n",
       "75%     2.000000  35.750000  1935.500000   2.000000  2000.000000   4.000000   \n",
       "max     3.000000  49.000000  1978.000000   3.000000  4558.000000   6.000000   \n",
       "\n",
       "         CLEAR-G     T-OR-D   MATERIAL       SPAN      REL-L       TYPE  \n",
       "count  70.000000  70.000000  70.000000  70.000000  70.000000  70.000000  \n",
       "mean    1.257143   1.814286   2.071429   1.742857   1.728571   3.457143  \n",
       "std     0.440215   0.391684   0.428054   0.629831   0.797122   1.575958  \n",
       "min     1.000000   1.000000   1.000000   1.000000   1.000000   1.000000  \n",
       "25%     1.000000   2.000000   2.000000   1.000000   1.000000   2.000000  \n",
       "50%     1.000000   2.000000   2.000000   2.000000   2.000000   4.000000  \n",
       "75%     1.750000   2.000000   2.000000   2.000000   2.000000   4.000000  \n",
       "max     2.000000   2.000000   3.000000   3.000000   3.000000   6.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_col=TARGET_COL diag_kind=\"kde\" kind=\"reg\"\n",
    "show_overall_dataset_scatter_plots(dataset, target_col=TARGET_COL, diag_kind=None, kind=\"reg\", corner=None, gmap_levels=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION']\n",
    "target_col = 'T-OR-D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_result = dataset.corr()\n",
    "# corr_result.head(corr_result.shape[0])\n",
    "display_heatmap(corr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_histograms_from_heatmap_corr_matrix(corr_result, row_names=dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make distinction between Target Variable and Predictors\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "columns = dataset.columns  # List of all attribute names\n",
    "target_col = 'T-OR-D'      # Target variable name\n",
    "\n",
    "# Get Target values and map to 0s and 1s\n",
    "y = np.array(list(map(lambda x: 0 if x == 1 else 1, dataset[target_col].values)))\n",
    "print('Summary about Target Variable {target_col}')\n",
    "print('-' * 50)\n",
    "print(dataset['T-OR-D'].value_counts())\n",
    "\n",
    "# Get Predictors\n",
    "X = dataset.loc[:, dataset.columns != target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "# --------------------------------------------------------------------------- #\n",
    "scaler_methods = ['minmax', 'standard', 'norm']\n",
    "scaler_method = 'standard'\n",
    "rescaledX = preprocessing_data_rescaling(scaler_method, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricipal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = rescaledX.shape[1]\n",
    "pca = PCA(n_components=n_components)\n",
    "# pca = PCA(n_components=2)\n",
    "\n",
    "# X_pca = pca.fit_transform(X)\n",
    "pca = pca.fit(rescaledX)\n",
    "X_pca = pca.transform(rescaledX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Cumulative varation explained(percentage) up to given number of pcs:\")\n",
    "\n",
    "tmp_data = []\n",
    "principal_components = [pc for pc in '2,5,6,7,8,9,10'.split(',')]\n",
    "for _, pc in enumerate(principal_components):\n",
    "    n_components = int(pc)\n",
    "    \n",
    "    cum_var_exp_up_to_n_pcs = np.cumsum(pca.explained_variance_ratio_)[n_components-1]\n",
    "    # print(f\"Cumulative varation explained up to {n_components} pcs = {cum_var_exp_up_to_n_pcs}\")\n",
    "    # print(f\"# pcs {n_components}: {cum_var_exp_up_to_n_pcs*100:.2f}%\")\n",
    "    tmp_data.append([n_components, cum_var_exp_up_to_n_pcs * 100])\n",
    "\n",
    "tmp_df = pd.DataFrame(data=tmp_data, columns=['# PCS', 'Cumulative Varation Explained (percentage)'])\n",
    "tmp_df.head(len(tmp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = rescaledX.shape[1]\n",
    "pca = PCA(n_components=n_components)\n",
    "# pca = PCA(n_components=2)\n",
    "\n",
    "#X_pca = pca.fit_transform(X)\n",
    "pca = pca.fit(rescaledX)\n",
    "X_pca = pca.transform(rescaledX)\n",
    "    \n",
    "fig = show_cum_variance_vs_components(pca, n_components)\n",
    "\n",
    "# py.sign_in('franec94', 'QbLNKpC0EZB0kol0aL2Z')\n",
    "# py.iplot(fig, filename='selecting-principal-components {}'.format(scaler_method))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Pros & Cons of PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models <a class=\"anchor\" id=\"learning-models\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be tested for Cross-Validation Approach\n",
    "\n",
    "estimators_list = [GaussianNB(), LogisticRegression(), KNeighborsClassifier(), SVC(), DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "estimators_names = ['GaussianNB', 'LogisticRegression', 'KNeighborsClassifier', 'SVC', 'DecisionTreeClassifier', 'RandomForestClassifier']\n",
    "\n",
    "pca_kernels_list = ['linear', 'poly', 'rbf', 'cosine',]\n",
    "cv_list = [10, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "\n",
    "parameters_sgd_classifier = {\n",
    "    'clf__loss': ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'),\n",
    "    'clf__penalty': ('l2', 'l1', 'elasticnet'),\n",
    "    'clf__alpha': (1e-1, 1e-2, 1e-3, 1e-4),\n",
    "    'clf__max_iter': (50, 100, 150, 200, 500, 1000, 1500, 2000, 2500),\n",
    "    'clf__learning_rate': ('optimal',),\n",
    "    'clf__tol': (None, 1e-2, 1e-4, 1e-5, 1e-6)\n",
    "}\n",
    "\n",
    "kernel_type = 'svm-rbf-kernel'\n",
    "parameters_svm = {\n",
    "    'clf__gamma': (0.003, 0.03, 0.05, 0.5, 0.7, 1.0, 1.5),\n",
    "    'clf__max_iter':(1e+2, 1e+3, 2 * 1e+3, 5 * 1e+3, 1e+4, 1.5 * 1e+3),\n",
    "    'clf__C': (1e-4, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3),\n",
    "}\n",
    "\n",
    "parmas_decision_tree = {\n",
    "    'clf__splitter': ('random', 'best'),\n",
    "    'clf__criterion':('gini', 'entropy'),\n",
    "    'clf__max_features': (None, 'auto', 'sqrt', 'log2')\n",
    "}\n",
    "\n",
    "parmas_random_forest = {\n",
    "    'clf__n_estimators': (3, 5, 7, 10, 30, 50, 70, 100, 150, 200),\n",
    "    'clf__criterion':('gini', 'entropy'),\n",
    "    'clf__bootstrap': (True, False)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=2)\n",
    "model.fit(X)              \n",
    "X_2D = model.transform(X)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['PCA1'] = X_2D[:, 0]\n",
    "df['PCA2'] = X_2D[:, 1]\n",
    "df[target_col] = dataset[target_col].values\n",
    "\n",
    "sns.lmplot(\"PCA1\", \"PCA2\", hue=target_col, data=df, fit_reg=False)\n",
    "\n",
    "# show_pca_1_vs_pca_2_pcaKernel(X, pca_kernels_list, target_col, dataset)\n",
    "# show_scatter_plots_pcaKernel(X, pca_kernels_list, target_col, dataset, n_components=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(estimators_list) # len(estimators_list)\n",
    "dfs_list = fit_all_by_n_components(\n",
    "    estimators_list=estimators_list[:n], \\\n",
    "    estimators_names=estimators_names[:n], \\\n",
    "    X=X, \\\n",
    "    y=y, \\\n",
    "    n_components=2, \\\n",
    "    show_plots=False, \\\n",
    "    cv_list=cv_list[:1], \\\n",
    "    # pca_kernels_list=['linear'],\n",
    "    pca_kernels_list=pca_kernels_list[:1],\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GaussianNB\n",
    "# -----------------------------------\n",
    "dfs_list[0].head(dfs_list[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "# -----------------------------------\n",
    "dfs_list[1].head(dfs_list[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "# -----------------------------------\n",
    "dfs_list[2].head(dfs_list[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DecisionTreeClassifier\n",
    "# -----------------------------------\n",
    "dfs_list[3].head(dfs_list[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "# -----------------------------------\n",
    "dfs_list[4].head(dfs_list[0].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements and Conclusions <a class=\"anchor\" id=\"Improvements-and-conclusions\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  <a class=\"anchor\" id=\"references\"></a>\n",
    "- Data Domain Information part:\n",
    "    - (Deck) https://en.wikipedia.org/wiki/Deck_(bridge)\n",
    "    - (Cantilever bridge) https://en.wikipedia.org/wiki/Cantilever_bridge\n",
    "    - (Arch bridge) https://en.wikipedia.org/wiki/Deck_(bridge)\n",
    "- Machine Learning part:\n",
    "    - (Theory Book) https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "    - (Decsion Trees) https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "    - (SVM) https://scikit-learn.org/stable/modules/svm.html\n",
    "    - (PCA) https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "- Chart part:\n",
    "    - (Seaborn Charts) https://acadgild.com/blog/data-visualization-using-matplotlib-and-seaborn\n",
    "- Markdown Math part:\n",
    "    - https://share.cocalc.com/share/b4a30ed038ee41d868dad094193ac462ccd228e2/Homework%20/HW%201.2%20-%20Markdown%20and%20LaTeX%20Cheatsheet.ipynb?viewer=share\n",
    "    - https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Typesetting%20Equations.html\n",
    "    \n",
    "#### others\n",
    "- Third Party Library:\n",
    "    - (statsmodels) https://www.statsmodels.org/stable/index.html#\n",
    "- KDE:\n",
    "    - (TUTORIAL) https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
