{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Space Report\n",
    "\n",
    "\n",
    "<img src=\"images/polito_logo.png\" alt=\"Polito Logo\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "## Pittsburgh Bridges Data Set\n",
    "\n",
    "<img src=\"images/andy_warhol_bridge.jpg\" alt=\"Andy Warhol Bridge\" style=\"width: 200px;\"/>\n",
    "\n",
    "    Andy Warhol Bridge - Pittsburgh.\n",
    "\n",
    "Report created by Student Francesco Maria Chiarlo s253666, for A.A 2019/2020.\n",
    "\n",
    "**Abstract**:The aim of this report is to evaluate the effectiveness of distinct, different statistical learning approaches, in particular focusing on their characteristics as well as on their advantages and backwards when applied onto a relatively small dataset as the one employed within this report, that is Pittsburgh Bridgesdataset.\n",
    "\n",
    "**Key words**:Statistical Learning, Machine Learning, Bridge Design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports Section <a class=\"anchor\" id=\"imports-section\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from utils.all_imports import *;\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global script variables\n",
    "# --------------------------------------------------------------------------- #\n",
    "dataset_path, dataset_name, column_names, TARGET_COL = \\\n",
    "    get_dataset_location() # Info Data to be fetched\n",
    "estimators_list, estimators_names = get_estimators() # Estimator to be trained\n",
    "\n",
    "dataset, feature_vs_values = load_brdiges_dataset(dataset_path, dataset_name)\n",
    "\n",
    "# variables used for pass through arrays used to store results\n",
    "pos_gs = 0; pos_cv = 0\n",
    "\n",
    "# Array used for storing graphs\n",
    "plots_names = list(map(lambda xi: f\"{xi}_learning_curve.png\", estimators_names))\n",
    "pca_kernels_list = ['linear', 'poly', 'rbf', 'cosine', 'sigmoid']\n",
    "cv_list = list(range(10, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-162d17a1bc70>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Make distinction between Target Variable and Predictors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# --------------------------------------------------------------------------- #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrescaledX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprepare_data_for_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTARGET_COL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Make distinction between Target Variable and Predictors\n",
    "# --------------------------------------------------------------------------- #\n",
    "rescaledX, y, columns = prepare_data_for_train(dataset, target_col=TARGET_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricipal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_table_pc_analysis(X=rescaledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Pros & Cons of PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models <a class=\"anchor\" id=\"learning-models\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be tested for Cross-Validation Approach\n",
    "# -----------------------------------------------------\n",
    "param_grids = []\n",
    "parmas_logreg = {\n",
    "    'penalty': ('l1', 'l2', 'elastic', None),\n",
    "    'solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
    "    'fit_intercept': (True, False),\n",
    "    'tol': (1e-4, 1e-3, 1e-2),\n",
    "    'class_weight': (None, 'balanced'),\n",
    "    'C': (10.0, 1.0, .1, .01, .001, .0001),\n",
    "    # 'random_state': (0,),\n",
    "}; param_grids.append(parmas_logreg)\n",
    "\n",
    "parmas_knn_clf = {\n",
    "    'n_neighbors': (2,3,4,5,6,7,8,9,10),\n",
    "    'weights': ('uniform', 'distance'),\n",
    "    'metric': ('euclidean', 'minkowski', 'manhattan'),\n",
    "    'leaf_size': (5, 10, 15, 30),\n",
    "    'algorithm': ('ball_tree', 'kd_tree', 'brute'),\n",
    "}; param_grids.append(parmas_knn_clf)\n",
    "\n",
    "params_sgd_clf = {\n",
    "    'loss': ('log', 'modified_huber'), # ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron')\n",
    "    'penalty': ('l2', 'l1', 'elasticnet'),\n",
    "    'alpha': (1e-1, 1e-2, 1e-3, 1e-4),\n",
    "    'max_iter': (50, 100, 150, 200, 500, 1000, 1500, 2000, 2500),\n",
    "    'class_weight': (None, 'balanced'),\n",
    "    'learning_rate': ('optimal',),\n",
    "    'tol': (None, 1e-2, 1e-4, 1e-5, 1e-6),\n",
    "    # 'random_state': (0,),\n",
    "}; param_grids.append(params_sgd_clf)\n",
    "\n",
    "kernel_type = 'svm-rbf-kernel'\n",
    "params_svm_clf = {\n",
    "    # 'gamma': (1e-7, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3, 1e+5, 1e+7),\n",
    "    'gamma': (1e-5, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3, 1e+5),\n",
    "    'max_iter':(1e+2, 1e+3, 2 * 1e+3, 5 * 1e+3, 1e+4, 1.5 * 1e+3),\n",
    "    'degree': (1,2,4,8),\n",
    "    'coef0': (.001, .01, .1, 0.0, 1.0, 10.0),\n",
    "    'shrinking': (True, False),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid',],\n",
    "    'class_weight': (None, 'balanced'),\n",
    "    'C': (1e-4, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3),\n",
    "    'probability': (True,),\n",
    "}; param_grids.append(params_svm_clf)\n",
    "\n",
    "parmas_tree = {\n",
    "    'splitter': ('random', 'best'),\n",
    "    'criterion':('gini', 'entropy'),\n",
    "    'max_features': (None, 'sqrt', 'log2'),\n",
    "    'max_depth': (None, 3, 5, 7, 10,),\n",
    "    'splitter': ('best', 'random',),\n",
    "    'class_weight': (None, 'balanced'),\n",
    "}; param_grids.append(parmas_tree)\n",
    "\n",
    "parmas_random_forest = {\n",
    "    'n_estimators': (3, 5, 7, 10, 30, 50, 70, 100, 150, 200),\n",
    "    'criterion':('gini', 'entropy'),\n",
    "    'bootstrap': (True, False),\n",
    "    'min_samples_leaf': (1,2,3,4,5),\n",
    "    'max_features': (None, 'sqrt', 'log2'),\n",
    "    'max_depth': (None, 3, 5, 7, 10,),\n",
    "    'class_weight': (None, 'balanced', 'balanced_subsample'),\n",
    "}; param_grids.append(parmas_random_forest)\n",
    "\n",
    "# Some variables to perform different tasks\n",
    "# -----------------------------------------------------\n",
    "N_CV, N_KERNEL, N_GS = 9, 5, 6;\n",
    "nrows = N_KERNEL // 2 if N_KERNEL % 2 == 0 else N_KERNEL // 2 + 1;\n",
    "ncols = 2; grid_size = [nrows, ncols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Learning Technique | Type of Learner | Type of Learning | Classification | Regression | Clustering |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| *Stochastic Gradient Descent (SGD)* | *Linear Model* | *Supervised Learning*| *Supported* | *Supported* | *Not-Supported*|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=9\n",
    "learning_curves_by_kernels(\n",
    "# learning_curves_by_components(\n",
    "    estimators_list[:], estimators_names[:],\n",
    "    rescaledX, y,\n",
    "    train_sizes=np.linspace(.1, 1.0, 10),\n",
    "    n_components=9,\n",
    "    pca_kernels_list=pca_kernels_list[0],\n",
    "    verbose=0,\n",
    "    by_pairs=True,\n",
    "    savefigs=True,\n",
    "    scoring='accuracy',\n",
    "    figs_dest=os.path.join('figures', 'learning_curve', f\"Pcs_{n_components}\"), ignore_func=True,\n",
    "    # figsize=(20,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dest = os.path.join(\"figures\", \"n_comp_9_analysis\", \"grid_search\")\n",
    "X = rescaledX; pos = 3\n",
    "\n",
    "df_gs, df_auc_gs, df_pvalue = grid_search_all_by_n_components(\n",
    "    estimators_list=estimators_list[pos], \\\n",
    "    param_grids=param_grids[pos - 1],\n",
    "    estimators_names=estimators_names[pos], \\\n",
    "    X=X, y=y,\n",
    "    n_components=9,\n",
    "    random_state=0, show_plots=False, show_errors=False, verbose=1, plot_dest=plot_dest, debug_var=False)\n",
    "df_9, df_9_auc = df_gs, df_auc_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results obtained running *Sgd Classifier* against our dataset splitted into training set and test set and adopting a different kernel trick applied to *kernel-Pca* unsupervised preprocessing method we can state generally speaking that looking at the weighted values of *Recall, Precision, and F1-Scores* we obtained good performance and and except for one trial where we got lower and worst results, when *Polynomial and Rbf Tricks* is selected, in the remaning cases have gotten remarkable results. More precisely we can say what follows:\n",
    "\n",
    "- speaking about __Linear kernel Pca based Sgd Classifier__, when adoping the default threshold of *.5* for classification purposes we have a model that reaches an accuracy of *65%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *79%* with a Roc Curve that shows a behavior for which the model increases its *TPR* without affecting the *FPR* score, however at  a given point the Roc Curve trend turns so that the two cited scores begin to increase linearly and with a slope lower than that of Random Classifier so that FPR increases faster. The model is very precise when predicting class 1 instances but it has a recall of just *54%* so misclassified more or less half of samples from class 1 and this fact influenced instead the precision of class 0 that is a bit low, just *32%*, while class 0 recall is very high. Since the test accuracy score loses nearly 30 percent points we can assume that sucha model quite overfit to train data, we are not really encouraged to adopt it except we decied to exploit it for including it in an ensemble classifier, more boosting like than bagging one.\n",
    "\n",
    "- observing __Polynomial kernel Pca based Sgd Estimator__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *76%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *73%*. It represents the best result obtained running th SGD based Training Algorithm upon our input dataset, in particular it obtained high precision and high recall for class 1, in other words such a model is able to recognize and correctly classify most of the data examples whose true label is indeed class 1. However, even if the model has high recall related to class 0, since the dataset is unbalanced we cannot say the same things for precision score about the class 0. So the model is somewhat uncertain when predicting class 0 as label  value for new observations.\n",
    "\n",
    "- review __Rbf kernel Pca based Sgd Classifier__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *82%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *57%*. In particular such a trial along with the *PCosine kernel Pca based Sgd Classifier* are the two attempts that lead to worts results, since the model overfit against the data employed at training time, but also the model gained weights that tend to predict every thing as class 1 instance. So, the resulting scores tell us that the model is highly precise and obtained high recall related to class 1, convercely has very low performance for precision and recall referred to class 0. Since such a model is performing just a little bit better than random classifier, can be largely adopted along other similar models for building voting classifier, following boosting like classifier policy and behavior.\n",
    "\n",
    "- looking at __Cosine kernel Pca based Sgd Classifier__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *32%* at test time against an accuracy of *95%* at train step, while the Auc score reaches a value of just *59%*.  Here the fine tuned model obtained from grid-search approach tells us that we are able to classify with high precision a few data examples from class 1, and even if we correctly classify all instances from class 0, we also wrongly predict class labels for most of instances,. whose true label is class 1. This means that the model is highly uncertain when predicting class 0 as the output target label. Moreover, the model's ROC Curve performs slighltly better than the random classifier, and we end up saying that such a model has gained weights and hyper-params that tend to predict the unknown instances as belonging to class 0 most of the time. We cannot neither say that switching the class labels between the two classes will allow us to obtain a better result since the roc curve trend is just a little bit better than the random classifier.\n",
    "\n",
    "- finally, referring to __Sigmoid kernel Pca based Sgd Model__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *44%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *66%*. This model behaves more or less as the model obtained from the first trial performed for Sgd-based classifier, so as the first model is slightly worst than the best model found here when adopting as classifier Sgd technique, that is the *Cosine kernel Pca based Sgd Classifier*.\n",
    "\n",
    "__Significance Analysis__: finally, when looking at the different graphics related to the test which aims at investigating the diagnostic power of our different models we have fine tuned for *SGD Classifier*, picking the best one for such a test we can notice that beacues of the *signficance level* $\\alpha$ set equal to *0.05 that is 5% of chance to reject the Null-Hypothesis $H_{0}$*, we have obtained following results. Adopting the SGD statistical learning technique for classification fine tuned as above with hyper-params selectd also depending on the kind of *kernel-trick adopted for kernel-Pca unsupervised technique*, we can calim that only two out of five trials lead to a *p-vlaue* worst than *selected significance level equal to 5%*, which are *Linear- and Cosine-kernel Pca based Sgd Classifier*, so rejecting the *Null-Hypotesis* for those two cases will results into a *Type I Error*. While the remaining three cases, that are *Poly-, Rbf- and Sigmoid-kernel Pca based Sgd Classifier* have obtained a p-value over the range $[.9, 3]$ *in percet points*, so we are satisfyed for the results obtained in terms of significance scores, however, only *Poly-, and Rbf-kernel Pca based Sgd Classifier* really matter or are worth models since they do not overfit too much and do not go worstly as *Sigmoid-kernel Pca based Sgd Classifier* at test time.\n",
    "\n",
    "#### Table Fine Tuned Hyper-Params(SGD Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_widget_list_df([df_gs, df_auc_gs]) #print(df_gs); print(df_auc_gs)\n",
    "show_table_summary_grid_search(df_gs, df_auc_gs, df_pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table dispalyed just above that shows the details about the selected values for hyper-parameters specified during grid search, in the different situations accordingly to the fixed kernel-trick for kernel Pca unsupervised method we can state that, referring to the first two columns of *Train and Test Accuracy*, we can recognize which trials lead to more overfit results such as for *Rbfd Trick* or less overfit solution such as in the case of *Linear, Polynomial, Cosine, and Sigmoid Tricks*. Speaking about the hyper-parameters, we can say what follows:\n",
    "\n",
    "- looking at __alpha hyper-parameter__, that is constant that multiplies the regularization term. The higher the value, the stronger the regularization. Also used to compute the learning rate when set to *learning_rate* is set to *'optimal'*, as was here, we can notice that the final choice through the different trials was more or less tha same, meanning that the adopted kernel trick for performing kernel-Pca does not affected appreciably such a hyper-param, which three cases out of five was set to *0.1*, and the remaining case adopted *0.0001*, *0.001* for respectively Cosine and Sigmoid based *kernel-Pca*. This also remind us that while training the classifiers was not necessary to force a high regularization contribute for reducing the overfit as well as the learning process, even if we know that *Rbf kernel Pca based Sgd Classifier* overfits mostly against train data, and gained weights that encourages predicting all samples as belonging to class 1.\n",
    "\n",
    "- reviewing __class_weight hyper-param__, what we can state about such a parameter is that it represents weights associated with classes. If not given, all classes are supposed to have weight one. The *“balanced” mode* uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as __n_samples / (n_classes * np.bincount(y))__. In particular we can notice that three out five models that were fine tuned accepted or selected *balanced weights*, which are *Linear-, Sigomoid-, Cosine-kernel Pca based Sgd Classifier*, while the remaining obtain better, when setting uniform weights which are models *Polynomial-, Rbf-kernel Pca based Sgd Classifier*. So the choiche of the right *kernel-trick* affected the subsequent selection at fine tuning time of the *class_weight hyper-param*. What we can further notice is that *Polynomial-, Rbf-kernel Pca based Sgd Classifier* more or less adopted same kind of values for hyper-params, as an instance for penalty hyper-param, however Polynomial model got worst performance in terms of accuracy but considering the other metrics simultaneously we can understand that the Poly model overfits less than Rbf one and so get better performance in general.\n",
    "\n",
    "- speaking of __learning_rate hyper-param__, since we force this as the unique available choice it was just report for completeness.\n",
    "\n",
    "- interesting it is the discussion about __loss parameter__, if fact we know that the possible options are *‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’*,  where the *‘log’ loss* gives logistic regression, a probabilistic classifier. *‘modified_huber’* is another smooth loss that brings tolerance to outliers as well as probability estimates. *‘squared_hinge’* is like hinge but is quadratically penalized. ‘perceptron’ is the linear loss used by the perceptron algorithm. Here, speaking about loss parameter we can clearly understand that the choice of a particular kernel trick does not affect the following choice of the loss function to be optimized, in fact uniformly all the models adopted or tend to prefer *modified_huber* loss function, allowing the models to fit to the data taking into account the fact that such a loss function is less sensitive to outliers, recalling inn fact that the Huber loss function is used in robust statistics, M-estimation and additive modelling. This loss is so cllaed beacues it derives from the plain version normally exploited for regression problems.\n",
    "\n",
    "- also when referring to __max iteration parameter__, we can easily say that thte models evenly adopted somewhat small number of iteration before stopping the learning procedure, this might be also becaues we work with a small dataset and so a set of data points that is small tend to overfit quickly and this migth be the reason for which in order to avoid too much overfit the training procedure performed employing grid-search technique for fine-tuning tend to prefer tiny number of iterations set for training the model.\n",
    "\n",
    "- __penalty parameter__, we recall here that it represents regularization term to be used. More precisely, defaults to *‘l2’* which is the standard regularizer for linear SVM models. *‘l1’* and *‘elasticnet’* might bring *sparsity* to the model (feature selection) not achievable with *‘l2’*. Also for such a hyper-param the choice of a particular *kernel-trick* to be used for *kernel-Pca* was affecting the subsequent selection of penalty contribute to regularize learning task, as was for *class weight hyper-param*. Here three over five models that are *Linear-, Sigomoid-, Cosine-kernel Pca based Sgd Classifier* adopted *l1-norm* as regularization term so the models's weights tend to be more sparse, while the remaining *Polynomial-, Rbf-kernel Pca based Sgd Classifier* models adopted *l2-nrom*. For the trials we have done, the models with *l1-regularization term* seem to get worst performance, more precisely *Sigomoid-, Cosine-kernel Pca based Sgd Classifier* even were worser than random classifier, while the *Linear-kernel Pca based Sgd Classifier* was slightly worst than Polynomial one, so does not overfit too much however we can say it can be exploited for ensemble method that follows a Boosting Policy.\n",
    "\n",
    "If we imagine to build up an *Ensemble Classifier* from the family of *Average Methods*, which state that the underlying principle leading their creation requires to build separate and single classifiers than averaging their prediction in regression context or adopting a majority vote strategy for the classification context, we can claim that amongst the purposed *Sgd classifier*, for sure, we could employ the classifier found from all the trials, except for *Rbf, Cosine and Sigmoid kernel Pca based Sgd Classifiers*, since the first model is overly overfitting to the data used at train time and more precisely most of the time predicted correctly just samples from class 1 and misclassifyes instances from class 0, the others instead assumed the opposite behavior. Also, because of their performance metrics and also because Ensemble Methods such as Bagging Classifier, usually work fine exploiting an ensemble of independent and fine tuned classifier differently from Boosting Methods which instead are based on weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_histogram_first_sample(Xtrain_transformed, ytrain_, estimators_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements and Conclusions <a class=\"anchor\" id=\"Improvements-and-conclusions\"></a>\n",
    "\n",
    "Extension that we can think of to better improve the analyses we can perform on such a relative tiny dataset many include, for preprocessing phases:\n",
    "- Selecting different *Feature Extraction ant Dimensionality Reduction Techniques* other than Pca or kernel Pca such as: \n",
    "*linear discriminant analysis (LDA)*, or *canonical correlation analysis (CCA) techniques* as a pre-processing step.\n",
    "\n",
    "Extension that we can think of to better improve the analyses we can perform on such a relative tiny dataset many include, for training phases:\n",
    "\n",
    "- Selecting different *Ensemble Methods, investigating both Average based and Boosting based Statistical Learning Methods*.\n",
    "\n",
    "Extension that we can think of to better improve the analyses we can perform on such a relative tiny dataset many include, for diagnostic analyses after having performed train and test phases:\n",
    "\n",
    "- Using other measures, indicators and ghraphical plots such as the *Total Operating Characteristic (TOC)*, since also such a measure characterizes diagnostic ability while revealing more information than the ROC. In fact for each threshold, ROC reveals two ratios, TP/(TP + FN) and FP/(FP + TN). In other words, ROC reveals hits/(hits + misses) and false alarms/(false alarms + correct rejections). On the other hand, TOC shows the total information in the contingency table for each threshold. Lastly, the TOC method reveals all of the information that the ROC method provides, plus additional important information that ROC does not reveal, i.e. the size of every entry in the contingency table for each threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References section  <a class=\"anchor\" id=\"references\"></a>\n",
    "### Main References\n",
    "- Data Domain Information part:\n",
    "    - (Deck) https://en.wikipedia.org/wiki/Deck_(bridge)\n",
    "    - (Cantilever bridge) https://en.wikipedia.org/wiki/Cantilever_bridge\n",
    "    - (Arch bridge) https://en.wikipedia.org/wiki/Deck_(bridge)\n",
    "- Machine Learning part:\n",
    "    - (Theory Book) https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "    - (Feature Extraction: PCA) https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "    - (Linear Model: Logistic Regression) https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    - (Neighbor-based Learning: Knn) https://scikit-learn.org/stable/modules/neighbors.html\n",
    "    - (Stochastc Learning: SGD Classifier) https://scikit-learn.org/stable/modules/sgd.html#sgd\n",
    "    - (Discriminative Model: SVM) https://scikit-learn.org/stable/modules/svm.html\n",
    "    - (Non-Parametric Learning: Decsion Trees) https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "    - (Ensemble, Non-Parametric Learning: RandomForest) https://scikit-learn.org/stable/modules/ensemble.html#forest\n",
    "- Metrics:\n",
    "    - (F1-Accuracy-Precision-Recall) https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\n",
    "- Statistics:\n",
    "    - (Correlation and dependence) https://en.wikipedia.org/wiki/Correlation_and_dependence\n",
    "    - (KDE) https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/\n",
    "- Chart part:\n",
    "    - (Seaborn Charts) https://acadgild.com/blog/data-visualization-using-matplotlib-and-seaborn\n",
    "- Third Party Library:\n",
    "    - (sklearn) https://scikit-learn.org/stable/index.html\n",
    "    - (statsmodels) https://www.statsmodels.org/stable/index.html#\n",
    "        \n",
    "### Others References\n",
    "- Plots:\n",
    "    - (Python Plot) https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=255798340456&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=1008025&gclid=Cj0KCQjw-_j1BRDkARIsAJcfmTFu4LAUDhRGK2D027PHiqIPSlxK3ud87Ek_lwOu8rt8A8YLrjFiHqsaAoLDEALw_wcB\n",
    "- Markdown Math part:\n",
    "    - (Math Symbols Latex) https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols\n",
    "    - (Tutorial 1) https://share.cocalc.com/share/b4a30ed038ee41d868dad094193ac462ccd228e2/Homework%20/HW%201.2%20-%20Markdown%20and%20LaTeX%20Cheatsheet.ipynb?viewer=share\n",
    "    - (Tutorial 2) https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Typesetting%20Equations.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
