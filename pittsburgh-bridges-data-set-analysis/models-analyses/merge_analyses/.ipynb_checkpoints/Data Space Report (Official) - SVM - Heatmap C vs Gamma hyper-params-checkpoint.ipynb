{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Space Report\n",
    "\n",
    "\n",
    "<img src=\"images/polito_logo.png\" alt=\"Polito Logo\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "## Pittsburgh Bridges Data Set\n",
    "\n",
    "<img src=\"images/andy_warhol_bridge.jpg\" alt=\"Andy Warhol Bridge\" style=\"width: 200px;\"/>\n",
    "\n",
    "    Andy Warhol Bridge - Pittsburgh.\n",
    "\n",
    "Report created by Student Francesco Maria Chiarlo s253666, for A.A 2019/2020.\n",
    "\n",
    "**Abstract**:The aim of this report is to evaluate the effectiveness of distinct, different statistical learning approaches, in particular focusing on their characteristics as well as on their advantages and backwards when applied onto a relatively small dataset as the one employed within this report, that is Pittsburgh Bridgesdataset.\n",
    "\n",
    "**Key words**:Statistical Learning, Machine Learning, Bridge Design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports Section <a class=\"anchor\" id=\"imports-section\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "# =========================================================================== #\n",
    "# STANDARD IMPORTS\n",
    "# =========================================================================== #\n",
    "print(__doc__)\n",
    "\n",
    "# Critical Imports\n",
    "# --------------------------------------------------------------------------- #\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Imports through 'from' syntax\n",
    "# --------------------------------------------------------------------------- #\n",
    "from pprint import pprint\n",
    "from itertools import islice\n",
    "from os import listdir; from os.path import isfile, join\n",
    "# Standard Imports\n",
    "# --------------------------------------------------------------------------- #\n",
    "import copy; import os\n",
    "import sys; import time\n",
    "import itertools\n",
    "import sklearn\n",
    "\n",
    "# Imports through 'as' syntax\n",
    "# --------------------------------------------------------------------------- #\n",
    "import numpy as np; import pandas as pd\n",
    "\n",
    "# Imports for handling graphics\n",
    "# --------------------------------------------------------------------------- #\n",
    "%matplotlib inline\n",
    "# Matplotlib pyplot provides plotting API\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import chart_studio.plotly.plotly as py\n",
    "import seaborn as sns;  sns.set(style=\"ticks\", color_codes=True) # sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================== #\n",
    "# UTILS IMPORTS (Done by myself)\n",
    "# =========================================================================== #\n",
    "from utils.load_dataset_pittsburg_utils import load_brdiges_dataset; from utils.display_utils import *\n",
    "from utils.preprocessing_utils import *; from utils.training_utils import *\n",
    "from utils.sklearn_functions_custom import *; from utils.learning_curves_custom import *\n",
    "from utils.training_utils_v2 import fit_by_n_components, fit_all_by_n_components, grid_search_all_by_n_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================== #\n",
    "# sklearn IMPORT\n",
    "# =========================================================================== #\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "# Import scikit-learn classes: models (Estimators).\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB   # Non-parametric Generative Model\n",
    "from sklearn.linear_model import LogisticRegression         # Parametric Linear Discriminative Model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC                          # Parametric Linear Discriminative \"Support Vector Classifier\"\n",
    "from sklearn.tree import DecisionTreeClassifier      # Non-parametric Model\n",
    "from sklearn.ensemble import RandomForestClassifier  # Non-parametric Model (Meta-Estimator, that is, an Ensemble Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================== #\n",
    "# READ INPUT DATASET\n",
    "# =========================================================================== #\n",
    "\n",
    "dataset_path = 'C:\\\\Users\\\\Francesco\\Documents\\\\datasets\\\\pittsburgh_dataset'\n",
    "dataset_name = 'bridges.data.csv'\n",
    "\n",
    "TARGET_COL = 'T-OR-D'  # Target variable name\n",
    "dataset, feature_vs_values = load_brdiges_dataset(dataset_path, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary about Target Variable {target_col}\n",
      "--------------------------------------------------\n",
      "2    57\n",
      "1    13\n",
      "Name: T-OR-D, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Make distinction between Target Variable and Predictors\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "columns = dataset.columns  # List of all attribute names\n",
    "\n",
    "# Get Target values and map to 0s and 1s\n",
    "# y = np.array(list(map(lambda x: 0 if x == 1 else 1, dataset[TARGET_COL].values)))\n",
    "y = np.array(list(map(lambda x: -1 if x == 1 else 1, dataset[TARGET_COL].values)))\n",
    "print('Summary about Target Variable {target_col}')\n",
    "print('-' * 50)\n",
    "print(dataset[TARGET_COL].value_counts())\n",
    "\n",
    "# Get Predictors\n",
    "X = dataset.loc[:, dataset.columns != TARGET_COL].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape features matrix X, after normalizing:  (70, 11)\n"
     ]
    }
   ],
   "source": [
    "# Standardizing the features\n",
    "# --------------------------------------------------------------------------- #\n",
    "scaler_methods = ['minmax', 'standard', 'norm']\n",
    "scaler_method = 'standard'\n",
    "rescaledX = preprocessing_data_rescaling(scaler_method, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricipal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = rescaledX.shape[1]\n",
    "pca = PCA(n_components=n_components)\n",
    "# pca = PCA(n_components=2)\n",
    "\n",
    "# X_pca = pca.fit_transform(X)\n",
    "pca = pca.fit(rescaledX)\n",
    "X_pca = pca.transform(rescaledX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative varation explained(percentage) up to given number of pcs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># PCS</th>\n",
       "      <th>Cumulative Varation Explained (percentage)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>47.738342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>75.856460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>82.615768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>88.413903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>92.661938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9</td>\n",
       "      <td>95.976841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>98.432807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # PCS  Cumulative Varation Explained (percentage)\n",
       "0      2                                   47.738342\n",
       "1      5                                   75.856460\n",
       "2      6                                   82.615768\n",
       "3      7                                   88.413903\n",
       "4      8                                   92.661938\n",
       "5      9                                   95.976841\n",
       "6     10                                   98.432807"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Cumulative varation explained(percentage) up to given number of pcs:\")\n",
    "\n",
    "tmp_data = []\n",
    "principal_components = [pc for pc in '2,5,6,7,8,9,10'.split(',')]\n",
    "for _, pc in enumerate(principal_components):\n",
    "    n_components = int(pc)\n",
    "    \n",
    "    cum_var_exp_up_to_n_pcs = np.cumsum(pca.explained_variance_ratio_)[n_components-1]\n",
    "    # print(f\"Cumulative varation explained up to {n_components} pcs = {cum_var_exp_up_to_n_pcs}\")\n",
    "    # print(f\"# pcs {n_components}: {cum_var_exp_up_to_n_pcs*100:.2f}%\")\n",
    "    tmp_data.append([n_components, cum_var_exp_up_to_n_pcs * 100])\n",
    "\n",
    "tmp_df = pd.DataFrame(data=tmp_data, columns=['# PCS', 'Cumulative Varation Explained (percentage)'])\n",
    "tmp_df.head(len(tmp_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Pros & Cons of PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models <a class=\"anchor\" id=\"learning-models\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be tested for Cross-Validation Approach\n",
    "\n",
    "estimators_list = [GaussianNB(), LogisticRegression(), KNeighborsClassifier(), SGDClassifier(), SVC(), DecisionTreeClassifier(), RandomForestClassifier()]\n",
    "estimators_names = ['GaussianNB', 'LogRegr', 'Knn', 'SGD', 'SVC', 'DecisionTree', 'RandomForest']\n",
    "plots_names = list(map(lambda xi: f\"{xi}_learning_curve.png\", estimators_names))\n",
    "\n",
    "pca_kernels_list = ['linear', 'poly', 'rbf', 'cosine', 'sigmoid']\n",
    "cv_list = [10, 9, 8, 7, 6, 5, 4, 3, 2]\n",
    "\n",
    "parmas_logistic_regression = {\n",
    "    'penalty': ('l1', 'l2', 'elastic', None),\n",
    "    'solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
    "    'fit_intercept': (True, False),\n",
    "    'tol': (1e-4, 1e-3, 1e-2),\n",
    "    'class_weight': (None, 'balanced'),\n",
    "    'C': (10.0, 1.0, .1, .01, .001, .0001),\n",
    "    'random_state': (0,),\n",
    "}\n",
    "\n",
    "\n",
    "parmas_knn_forest = {\n",
    "    'n_neighbors': (2,3,4,5,6,7,8,9,10),\n",
    "    'weights': ('uniform', 'distance'),\n",
    "    'metric': ('euclidean', 'minkowski', 'manhattan'),\n",
    "    'leaf_size': (5, 10, 15, 30),\n",
    "    'algorithm': ('ball_tree', 'kd_tree', 'brute'),\n",
    "}\n",
    "\n",
    "parameters_sgd_classifier = {\n",
    "    'loss': ('log', 'modified_huber'), # ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron')\n",
    "    'penalty': ('l2', 'l1', 'elasticnet'),\n",
    "    'alpha': (1e-1, 1e-2, 1e-3, 1e-4),\n",
    "    'max_iter': (50, 100, 150, 200, 500, 1000, 1500, 2000, 2500),\n",
    "    'learning_rate': ('optimal',),\n",
    "    'tol': (None, 1e-2, 1e-4, 1e-5, 1e-6)\n",
    "}\n",
    "\n",
    "kernel_type = 'svm-rbf-kernel'\n",
    "parameters_svm = {\n",
    "    # 'gamma': (.003, .03, .05, .5, .7, 1.0, 1.5, 10.0, 15.0),\n",
    "    # 'gamma': (1e-7, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3, 1e+5, 1e+7),\n",
    "    # 'gamma': (.003, .03, .05, .5, .7, 1.0, 1.5),\n",
    "    # 'max_iter':(1e+2, 1e+3, 2 * 1e+3, 5 * 1e+3, 1e+4, 1.5 * 1e+3),\n",
    "    # 'penalty': ('l2','l1'),\n",
    "    # 'degree': (1,2,3,4,5,6,7,8,9,10),\n",
    "    # 'degree': (1,2,3,4),\n",
    "    # 'coef0': (.001, .01, .1, 0.0, 1.0, 10.0),\n",
    "    # 'shrinking': (True, False),\n",
    "    # 'kernel': ['linear', 'poly', 'rbf', 'sigmoid',],\n",
    "    'class_weight': (None, 'balanced'),\n",
    "    # 'C': (1e-7, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3, 1e+5, 1e+7),\n",
    "    'probability': (True,), \n",
    "}\n",
    "\n",
    "parmas_decision_tree = {\n",
    "    'splitter': ('random', 'best'),\n",
    "    'criterion':('gini', 'entropy'),\n",
    "    'max_features': (None, 'auto', 'sqrt', 'log2')\n",
    "}\n",
    "\n",
    "parmas_random_forest = {\n",
    "    'n_estimators': (3, 5, 7, 10, 30, 50, 70, 100, 150, 200),\n",
    "    'criterion':('gini', 'entropy'),\n",
    "    'bootstrap': (True, False)\n",
    "}\n",
    "\n",
    "param_grids = [parmas_logistic_regression, parmas_knn_forest, parameters_sgd_classifier, parameters_svm, parmas_decision_tree, parmas_random_forest]\n",
    "\n",
    "N_CV, N_KERNEL, N_GS = 9, 4, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=9\n",
    "learning_curves_by_kernels(\n",
    "# learning_curves_by_components(\n",
    "    estimators_list[:], estimators_names[:],\n",
    "    rescaledX, y,\n",
    "    train_sizes=np.linspace(.1, 1.0, 10),\n",
    "    n_components=9,\n",
    "    pca_kernels_list=pca_kernels_list[0],\n",
    "    verbose=0,\n",
    "    by_pairs=True,\n",
    "    savefigs=True,\n",
    "    scoring='accuracy',\n",
    "    figs_dest=os.path.join('figures', 'learning_curve', f\"Pcs_{n_components}\"), ignore_func=True,\n",
    "    # figsize=(20,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel PCA: Linear | SVC\n",
      "====================================================================================================\n",
      "Kernel PCA: Poly | SVC\n",
      "====================================================================================================\n",
      "Kernel PCA: Rbf | SVC\n",
      "====================================================================================================\n",
      "Kernel PCA: Cosine | SVC\n",
      "====================================================================================================\n",
      "Kernel PCA: Sigmoid | SVC\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "plot_dest = os.path.join(\"figures\", \"n_comp_9_analysis\", \"grid_search\")\n",
    "X = rescaledX\n",
    "\n",
    "df_gs, df_auc_gs = grid_search_all_by_n_components(\n",
    "    estimators_list=estimators_list[4], \\\n",
    "    param_grids=param_grids[3],\n",
    "    estimators_names=estimators_names[4], \\\n",
    "    X=X, y=y,\n",
    "    n_components=9,\n",
    "    random_state=0, show_plots=False, show_errors=True, verbose=1, plot_dest=plot_dest, debug_var=False, flag_no_computation=False)\n",
    "df_9, df_9_auc = df_gs, df_auc_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results obtained running *Knn Classifier* against our dataset splitted into training set and test set and adopting a different kernel trick applied to *kernel-Pca* unsupervised preprocessing method we can state generally speaking that all the such a *Statistical Learning technique* leads to a sequence of results that on average are more than appreciable beacues of the accuracy scores obtained at test time which compared against the same score but related to train phase allows us to understand that during the model creation the resulting classifiers do not overfit to the data, and even when the training score was high it was still lower than the scores in terms of accuracy obtained from the Logisti Regression Models which overfit to the data. Moreover, looking at the weighted values of *Recall, Precision, and F1-Scores* we can notably claim that the classifiers based on Knn obtained good performance and and except for one trial where we got lower and worst results, when *Sigmoid Trick* is selected, in the remaning cases have gotten remarkable results. More precisely we can say what follows:\n",
    "- speaking about __Linear kernel Pca based Knn Classifier__, when adoping the default threshold of *.5* for classification purposes we have a model that reaches an accuracy of *71%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *76%* with a Roc Curve that shows a behavior for which the model...\n",
    "\n",
    "- observing __Polynomial kernel Pca based Knn Estimator__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *74%* at test time against an accuracy of *89%* at train step, while the Auc score reaches a value of *77%*...\n",
    "\n",
    "- review __Rbf kernel Pca based Knn Classifier__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *79%* at test time against an accuracy of *95%* at train step, while the Auc score reaches a value of *74%*...\n",
    "\n",
    "- looking at __Cosine kernel Pca based Knn Classifier__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *59%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *62%*...\n",
    "\n",
    "- finally, referring to __Sigmoid kernel Pca based Knn Model__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *65%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *72%*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06fd8da6ceab492580c5b7d89865dab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Output(), Output()))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "create_widget_list_df([df_gs, df_auc_gs]) #print(df_gs); print(df_auc_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table dispalyed just above that shows the details about the selected values for hyper-parameters specified during grid search, in the different situations accordingly to the fixed kernel-trick for kernel Pca unsupervised method we can state that, referring to the first two columns of *Train and Test Accuracy*, we can recognize which trials lead to more overfit results such as for *Linear, Polynomial, and Sigmoid Tricks* or less overfit solution such as in the case of *Cosine, Trick*. Speaking about the hyper-parameters, we can say what follows:\n",
    "\n",
    "- referring to the svm's __Gamma Parameter__, which we recall that corresponds normally to *Kernel Coefficient* for *rbf, poly and sigmoid reprojection strategies*, we also remind that intuitively, the gamma parameter defines how far the influence of a single training example reaches, with *low values* meaning *far* and *high values* meaning *close*. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    "- looking at svm's __C Parameter__, and suggesting that such a parameter briefly trades off correct classification of training examples against maximization of the decision functionâ€™s margin. For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. So that, a lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words ``C`` behaves as a regularization parameter in the SVM.\n",
    "\n",
    "- instead, when speaking about svm's __Degree Parameter__, which is degree of the *polynomial kernel function (poly)*, therefore ignored by all other kernels, we are talking about a niche hyper-params that usually taking larger values implyes attempting to fit models that tends to lower bias values and higher variance, instead taking into account lower polynomial order my bit on higher bias and lower variance models. In other words, in one case we attempt to explain with a more fancy model a problem that might not follow a linear and smooth trend, vice versa, in the second case we prefer to represent our model by means of a lower capaciy classifier, easier to explain and understand.\n",
    "- considering svm's __coef0 Parameter__, which is Independent term in kernel function. It is only significant in svm kernel tricks' *poly and sigmoid* ...\n",
    "\n",
    "- the svm's __Shrinking Parameter__, from research field we found that if the number of iterations is large, then shrinking can shorten the training time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements and Conclusions <a class=\"anchor\" id=\"Improvements-and-conclusions\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References  <a class=\"anchor\" id=\"references\"></a>\n",
    "- Data Domain Information part:\n",
    "    - (Deck) https://en.wikipedia.org/wiki/Deck_(bridge)\n",
    "    - (Cantilever bridge) https://en.wikipedia.org/wiki/Cantilever_bridge\n",
    "    - (Arch bridge) https://en.wikipedia.org/wiki/Deck_(bridge)\n",
    "- Machine Learning part:\n",
    "    - (Theory Book) https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "    - (Decsion Trees) https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "    - (SVM) https://scikit-learn.org/stable/modules/svm.html\n",
    "    - (PCA) https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "- Chart part:\n",
    "    - (Seaborn Charts) https://acadgild.com/blog/data-visualization-using-matplotlib-and-seaborn\n",
    "- Markdown Math part:\n",
    "    - https://share.cocalc.com/share/b4a30ed038ee41d868dad094193ac462ccd228e2/Homework%20/HW%201.2%20-%20Markdown%20and%20LaTeX%20Cheatsheet.ipynb?viewer=share\n",
    "    - https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Typesetting%20Equations.html\n",
    "    \n",
    "#### others\n",
    "- Plots:\n",
    "    - (Python Plot) https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=255798340456&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=1008025&gclid=Cj0KCQjw-_j1BRDkARIsAJcfmTFu4LAUDhRGK2D027PHiqIPSlxK3ud87Ek_lwOu8rt8A8YLrjFiHqsaAoLDEALw_wcB\n",
    "- Third Party Library:\n",
    "    - (statsmodels) https://www.statsmodels.org/stable/index.html#\n",
    "- KDE:\n",
    "    - (TUTORIAL) https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/\n",
    "- Metrics:\n",
    "    - (F1-Accuracy-Precision-Recall) https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
