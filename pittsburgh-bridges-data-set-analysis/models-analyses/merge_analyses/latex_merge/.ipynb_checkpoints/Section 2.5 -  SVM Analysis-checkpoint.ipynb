{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines Classifier\n",
    "\n",
    "<img src=\"images/SVM_margin.png\" alt=\"SVM Margin \" style=\"width: 200px;\"/>\n",
    "\n",
    "| Learning Technique | Type of Learner | Type of Learning | Classification | Regression | Clustering | Outlier Detection \n",
    "| --- | --- | --- | --- | --- | --- | --- |\n",
    "| *Support Vector Machines(SVMs)* | *Discriminative Model* | *Supervised Learning*| *Supported* | *Supported* | *Not-Supported* | *Supported* |\n",
    "\n",
    "Here, in this section I'm going to exploit a machine learning techinique known as Support Vectors Machines in order to detect and so select the best model I can produce throughout the usage of data points contained within the dataset at hand. So let discuss a bit those kind of classifiers.\n",
    "\n",
    "In machine learning, **support-vector machines**, shortly SVMs, are *supervised learning models* with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a *non-probabilistic binary linear classifier*. An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.\n",
    "\n",
    "More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high-dimensional space, which can be used for classification, regression. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class, so-called *functional margin*, since in general the larger the margin, the lower the *generalization error* of the classifier.\n",
    "\n",
    "#### Mathematical formulation of SVMs\n",
    "Here, I'm going to describe the main mathematical properties and characteristics used to derive from a math point of view the algorithm derived and proven by researches when they have studied SVMs classifiers.\n",
    "\n",
    "I start saying and recalling that A support vector machine constructs a hyper-plane or set of hyper-planes in a high or infinite dimensional space, which can be used for classification, regression or other tasks. Intuitively, a good separation is achieved by the hyper-plane that has the largest distance to the nearest training data points of any class so-called functional margin, since in general the larger the margin the lower the generalization error of the classifier.\n",
    "\n",
    "When demonstrating SVMs classifier algorithm I suppose that We are given a training dataset of *n*n points of the form:\n",
    "\n",
    "\\begin{align}\n",
    "(\\vec{x_1} y_1),..,(\\vec{x_n},y_n)\n",
    "\\end{align}\n",
    "\n",
    "where the $y_{1}$ are either 1 or −1, each indicating the class to which the point $\\vec{x}_{i}$ belongs. Each $\\vec{x}_{i}$is a *p-dimensional real vector*. We want to find the \"maximum-margin hyperplane\" that divides the group of points $\\vec{x}_{i}$ for which $y_{1}$ = 1from the group of points for which $y_{1}$ = − 1, which is defined so that the distance between the hyperplane and the nearest point $\\vec{x}_{i}$  from either group is maximized.\n",
    "\n",
    "Any hyperplane can be written as the set of points $\\vec{x}_{i}$  satisfying : $\\vec{w}_{i}\\cdot{\\vec{x}_{i}} - b = 0$; where  $\\vec{w}_{i}$ is the, even if not necessarily, normal vector to the hyperplane. The parameter $\\tfrac {b}{\\|\\vec{w}\\|}$ determines the offset of the hyperplane from the origin along the normal vector $\\vec{x}_{i}$.\n",
    "\n",
    "Arrived so far, I have to distiguish between two distinct cases which both depende on the nature of data points that generally made up a given dataset. Those two different cases are called *Hard-Margin* and *Soft Margin* and, respectively.\n",
    "\n",
    "The first case, so the ***Hard-Margin*** case, happens just for really optimistics datasets. In fact it is the case when the training data is linearly separable, hence, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible. The region bounded by these two hyperplanes is called the \"margin\", and the maximum-margin hyperplane is the hyperplane that lies halfway between them. With a normalized or standardized dataset, these hyperplanes can be described by the equations:\n",
    "- $\\vec{w}_{i}\\cdot{\\vec{x}_{i}} - b = 1$, that is anything on or above this boundary is of one class, with label 1;\n",
    "- and, $\\vec{w}_{i}\\cdot{\\vec{x}_{i}} - b = -1$, that is anything on or above this boundary is of one class, with label -1.\n",
    "\n",
    "We can notice also that the distance between these two hyperplanes is ${\\displaystyle {\\tfrac {2}{\\|{\\vec {w}}\\|}}}$ so to maximize the distance between the planes we want to minimize ‖ w → ‖ {\\displaystyle \\|{\\vec {w}}\\|} \\|{\\vec {w}}\\|. The distance is computed using the distance from a point to a plane equation. We also have to prevent data points from falling into the margin, we add the following constraint: for each *i*:\n",
    "- either, ${\\displaystyle {\\vec {w}}\\cdot {\\vec {x}}_{i}-b\\geq 1}$, ${\\displaystyle y_{i}=1}$;\n",
    "- or, ${\\displaystyle {\\vec {w}}\\cdot {\\vec {x}}_{i}-b\\leq -1}$, if ${\\displaystyle y_{i}=-1}$.\n",
    "\n",
    "These constraints state that each data point must lie on the correct side of the margin.\n",
    "Finally, we collect all the previous observations and define the following optimization problem:\n",
    "- from $y_{i}(\\vec{w}_{i}\\cdot{\\vec{x}_{i}} - b) \\geq 1$, for all $1 \\leq i \\leq n$;\n",
    "- to minimize ${\\displaystyle y_{i}({\\vec {w}}\\cdot {\\vec {x}}_{i}-b)\\geq 1}$ ${\\displaystyle i=1,\\ldots ,n}$.\n",
    "\n",
    "The classifier we obtain is made from ${\\vec {w}}$ and ${\\displaystyle b}$ that solve this problem, and he max-margin hyperplane is completely determined by those ${\\vec {x}}_{i}$ that lie nearest to it. These $\\vec{x}_{i}$ are called *support vectors*.\n",
    "\n",
    "The other case, so the ***Soft-Margin*** case, convercely happens when the training data is not linearly separable. To deal with such situation, as well as, to extend SVM to cases in which the data are not linearly separable, we introduce the hinge loss function, that is: $max(y_{i}(\\vec{w}_{i}\\cdot{\\vec{x}_{i}} - b))$.\n",
    "Once we have provided the new loss function we go ahead with the new optimization problem that we aim at minimizing that is:\n",
    "\n",
    "\\begin{align}\n",
    "{\\displaystyle \\left[{\\frac {1}{n}}\\sum _{i=1}^{n}\\max \\left(0,1-y_{i}({\\vec {w}}\\cdot {\\vec {x}}_{i}-b)\\right)\\right]+\\lambda \\lVert {\\vec {w}}\\rVert ^{2},}\n",
    "\\end{align}\n",
    "\n",
    "where the parameter \\lambda determines the trade-off between increasing the margin size and ensuring that the ${\\vec {x}}_{i}$ lie on the correct side of the margin. Thus, for sufficiently small values of\\lambda , the second term in the loss function will become negligible, hence, it will behave similar to the hard-margin SVM, if the input data are linearly classifiable, but will still learn if a classification rule is viable or not.\n",
    "\n",
    "What we notice from last equations written just above is that we deal with a quadratic programming problem, and its solution is provided, detailed below.\n",
    "\n",
    "We start defining a *Primal Problem* as follow:\n",
    "- For each $\\{1,\\,\\ldots ,\\,n\\}$ we introduce a variable ${\\displaystyle \\zeta _{i}=\\max \\left(0,1-y_{i}(w\\cdot x_{i}-b)\\right)}$. Note that ${\\displaystyle \\zeta _{i}}$ is the smallest nonnegative number satisfying ${\\displaystyle y_{i}(w\\cdot x_{i}-b)\\geq 1-\\zeta _{i}}$;\n",
    "- we can rewrite the optimization problem as follows: ${\\displaystyle {\\text{minimize }}{\\frac {1}{n}}\\sum _{i=1}^{n}\\zeta _{i}+\\lambda \\|w\\|^{2}}$,  ${\\displaystyle {\\text{subject to }}y_{i}(w\\cdot x_{i}-b)\\geq 1-\\zeta _{i}\\,{\\text{ and }}\\,\\zeta _{i}\\geq 0,\\,{\\text{for all }}i.}$\n",
    "\n",
    "However, by solving for the *Lagrangian dual* of the above problem, one obtains the simplified problem:\n",
    "\\begin{align}\n",
    "    {\\displaystyle {\\text{maximize}}\\,\\,f(c_{1}\\ldots c_{n})=\\sum _{i=1}^{n}c_{i}-{\\frac {1}{2}}\\sum _{i=1}^{n}\\sum _{j=1}^{n}y_{i}c_{i}(x_{i}\\cdot x_{j})y_{j}c_{j},} \\\\\n",
    "{\\displaystyle {\\text{subject to }}\\sum _{i=1}^{n}c_{i}y_{i}=0,\\,{\\text{and }}0\\leq c_{i}\\leq {\\frac {1}{2n\\lambda }}\\;{\\text{for all }}i.} \n",
    "\\end{align}\n",
    "- moreover, the variables $c_i$ are defined as ${\\displaystyle {\\vec {w}}=\\sum _{i=1}^{n}c_{i}y_{i}{\\vec {x}}_{i}}$. Where, ${\\displaystyle c_{i}=0}$ exactly when ${\\displaystyle {\\vec {x}}_{i}}$ lies on the correct side of the margin, and ${\\displaystyle 0<c_{i}<(2n\\lambda )^{-1}}$ when ${\\vec {x}}_{i}$ lies on the margin's boundary. It follows that ${\\displaystyle {\\vec {w}}}$ can be written as a linear combination of the support vectors.\n",
    "\n",
    "The offset, ${\\displaystyle b}$, can be recovered by finding an ${\\vec {x}}_{i}$ on the margin's boundary and solving: ${\\displaystyle y_{i}({\\vec {w}}\\cdot {\\vec {x}}_{i}-b)=1\\iff b={\\vec {w}}\\cdot {\\vec {x}}_{i}-y_{i}.}$ \n",
    "\n",
    "This is called the *dual problem*. Since the dual maximization problem is a quadratic function of the ${\\displaystyle c_{i}}$ subject to linear constraints, it is efficiently solvable by quadratic programming algorithms.\n",
    "\n",
    "Lastly, I will discuss what in the context of SVMs classifier is called as ***Kernel Trick***.\n",
    "Roughly speaking, we know that a possible way of dealing with datasets that are not linearly separable but that can become linearnly separable within an higher dimensional space, or feature space, we can try to remap the original data points into a higher order feature space by means of some kind of remapping function, hence, solve the SVMs classifier optimization problem to find out a linear classifier in that new larger feature space. Then, we project back to the original feature space the solution we have found, reminding that in the hold feature space the decision boundaries founded will be non-linear, but still allow to classify new examples.\n",
    "\n",
    "Usually, especially, dealing with large datasets or with datasets with large set of features this approach becomes computationally intensive and and unfeasible if we run out of memory. So, in other words, the procedure is constrained in time and space, and might become time consuming or even unfeasible because of the large amount of memory we have to exploit.\n",
    "\n",
    "An reasonable alternative is represented by the usage of kernel functions that are function which satisfy ${\\displaystyle k({\\vec {x}}_{i},{\\vec {x}}_{j})=\\varphi ({\\vec {x}}_{i})\\cdot \\varphi ({\\vec {x}}_{j})}$, where we recall that classification vector ${\\vec {w}}$ in the transformed space satisfies ${\\displaystyle {\\vec {w}}=\\sum _{i=1}^{n}c_{i}y_{i}\\varphi ({\\vec {x}}_{i}),}$\n",
    "where, the ${\\displaystyle c_{i}}$ are obtained by solving the optimization problem:\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}{\\text{maximize}}\\,\\,f(c_{1}\\ldots c_{n})&=\\sum _{i=1}^{n}c_{i}-{\\frac {1}{2}}\\sum _{i=1}^{n}\\sum _{j=1}^{n}y_{i}c_{i}(\\varphi ({\\vec {x}}_{i})\\cdot \\varphi ({\\vec {x}}_{j}))y_{j}c_{j}\\\\&=\\sum _{i=1}^{n}c_{i}-{\\frac {1}{2}}\\sum _{i=1}^{n}\\sum _{j=1}^{n}y_{i}c_{i}k({\\vec {x}}_{i},{\\vec {x}}_{j})y_{j}c_{j}\\\\\\end{aligned}}}$\n",
    "\n",
    "${\\displaystyle {\\text{subject to }}\\sum _{i=1}^{n}c_{i}y_{i}=0,\\,{\\text{and }}0\\leq c_{i}\\leq {\\frac {1}{2n\\lambda }}\\;{\\text{for all }}i.}$\n",
    "\n",
    "The coefficients ${\\displaystyle c_{i}}$ can be solved for using quadratic programming, and we can find some index ${\\displaystyle i}$ such that ${\\displaystyle 0<c_{i}<(2n\\lambda )^{-1}}$, so that ${\\displaystyle \\varphi ({\\vec {x}}_{i})}$ lies on the boundary of the margin in the transformed space, and then solve, by substituting doto product between remapped data points with kernel function applied upon the same arguments:\n",
    "\n",
    "${\\displaystyle {\\begin{aligned}b={\\vec {w}}\\cdot \\varphi ({\\vec {x}}_{i})-y_{i}&=\\left[\\sum _{j=1}^{n}c_{j}y_{j}\\varphi ({\\vec {x}}_{j})\\cdot \\varphi ({\\vec {x}}_{i})\\right]-y_{i}\\\\&=\\left[\\sum _{j=1}^{n}c_{j}y_{j}k({\\vec {x}}_{j},{\\vec {x}}_{i})\\right]-y_{i}.\\end{aligned}}}$\n",
    "\n",
    "Finally, ${\\displaystyle {\\vec {z}}\\mapsto \\operatorname {sgn}({\\vec {w}}\\cdot \\varphi ({\\vec {z}})-b)=\\operatorname {sgn} \\left(\\left[\\sum _{i=1}^{n}c_{i}y_{i}k({\\vec {x}}_{i},{\\vec {z}})\\right]-b\\right).}$\n",
    "\n",
    "What follows is a briefly list of the most commonly used kernel functions. They should be fine tuned, by means of a either grid search or random search approaches, identifying the best set of values to replace whitin the picked kernel function, where the choice depend on the dataset at hand:\n",
    "- Polynomial (homogeneous): ${\\displaystyle k({\\vec {x_{i}}},{\\vec {x_{j}}})=({\\vec {x_{i}}}\\cdot {\\vec {x_{j}}})^{d}}$.\n",
    "- Polynomial (inhomogeneous): ${\\displaystyle k({\\vec {x_{i}}},{\\vec {x_{j}}})=({\\vec {x_{i}}}\\cdot {\\vec {x_{j}}}+1)^{d}}$.\n",
    "- Gaussian radial basis function: ${\\displaystyle \\gamma =1/(2\\sigma ^{2})} {\\displaystyle \\gamma =1/(2\\sigma ^{2})}$.\n",
    "- Hyperbolic tangent: ${\\displaystyle k({\\vec {x_{i}}},{\\vec {x_{j}}})=\\tanh(\\kappa {\\vec {x_{i}}}\\cdot {\\vec {x_{j}}}+c)}$ for some (not every) ${\\displaystyle \\kappa >0}$ and ${\\displaystyle c<0}$.\n",
    "\n",
    "What follows is the application or use of SVMs classifier for learning a model that best fit the training data in order to be able to classify new instance in a reliable way, selecting the most promising model trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from utils.all_imports import *;\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for notebook repeatability\n",
    "np.random.seed(0)\n",
    "\n",
    "# READ INPUT DATASET\n",
    "# --------------------------------------------------------------------------- #\n",
    "dataset_path, dataset_name, column_names, TARGET_COL = get_dataset_location()\n",
    "estimators_list, estimators_names = get_estimators()\n",
    "\n",
    "dataset, feature_vs_values = load_brdiges_dataset(dataset_path, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary about Target Variable {target_col}\n",
      "--------------------------------------------------\n",
      "2    57\n",
      "1    13\n",
      "Name: T-OR-D, dtype: int64\n",
      "shape features matrix X, after normalizing:  (70, 11)\n"
     ]
    }
   ],
   "source": [
    "# Make distinction between Target Variable and Predictors\n",
    "# --------------------------------------------------------------------------- #\n",
    "rescaledX, y, columns = prepare_data_for_train(dataset, target_col=TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be tested for Cross-Validation Approach\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Array used for storing graphs\n",
    "plots_names = list(map(lambda xi: f\"{xi}_learning_curve.png\", estimators_names))\n",
    "pca_kernels_list = ['linear', 'poly', 'rbf', 'cosine', 'sigmoid']\n",
    "cv_list = list(range(10, 1, -1))\n",
    "\n",
    "param_grids = []\n",
    "kernel_type = 'svm-rbf-kernel'\n",
    "params_svm_clf = {\n",
    "    # 'gamma': (1e-7, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3, 1e+5, 1e+7),\n",
    "    'gamma': (1e-5, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3, 1e+5),\n",
    "    'max_iter':(50, 100, 200, 500, 1e+3,),\n",
    "    'degree': (1,2,4,8),\n",
    "    'coef0': (.001, .01, .1, 0.0, 1.0, 10.0),\n",
    "    'shrinking': (True, False),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid',],\n",
    "    'class_weight': (None, 'balanced'),\n",
    "    'C': (1e-4, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3),\n",
    "    'probability': (True,),\n",
    "}; param_grids.append(params_svm_clf)\n",
    "# Some variables to perform different tasks\n",
    "# -----------------------------------------------------\n",
    "N_CV, N_KERNEL, N_GS = 9, 5, 6;\n",
    "nrows = N_KERNEL // 2 if N_KERNEL % 2 == 0 else N_KERNEL // 2 + 1;\n",
    "ncols = 2; grid_size = [nrows, ncols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=9\n",
    "learning_curves_by_kernels(\n",
    "# learning_curves_by_components(\n",
    "    estimators_list[:], estimators_names[:],\n",
    "    rescaledX, y,\n",
    "    train_sizes=np.linspace(.1, 1.0, 10),\n",
    "    n_components=9,\n",
    "    pca_kernels_list=pca_kernels_list[0],\n",
    "    verbose=0,\n",
    "    by_pairs=True,\n",
    "    savefigs=True,\n",
    "    scoring='accuracy',\n",
    "    figs_dest=os.path.join('figures', 'learning_curve', f\"Pcs_{n_components}\"), ignore_func=True,\n",
    "    # figsize=(20,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernel PCA: Linear | SVC\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "plot_dest = os.path.join(\"figures\", \"n_comp_9_analysis\", \"grid_search\")\n",
    "X = rescaledX\n",
    "\n",
    "df_gs, df_auc_gs, df_pvalue = grid_search_all_by_n_components(\n",
    "    estimators_list=estimators_list[4], \\\n",
    "    param_grids=param_grids[0],\n",
    "    estimators_names=estimators_names[4], \\\n",
    "    X=X, y=y,\n",
    "    n_components=9,\n",
    "    random_state=0, show_plots=False, show_errors=False, verbose=1, plot_dest=plot_dest, debug_var=False)\n",
    "df_9, df_9_auc = df_gs, df_auc_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results obtained running *Svm Classifier* against our dataset splitted into training set and test set and adopting a different kernel trick applied to *kernel-Pca* unsupervised preprocessing method we can state generally speaking that all the such a *Statistical Learning technique* leads to a sequence of results that on average are more than appreciable beacues of the accuracy scores obtained at test time which compared against the same score but related to train phase allows us to understand that during the model creation the resulting classifiers do not overfit to the data, and even when the training score was high it was still lower than the scores in terms of accuracy obtained from the Logisti Regression Models which overfit to the data. Moreover, looking at the weighted values of *Recall, Precision, and F1-Scores* we can notably claim that the classifiers based on Knn obtained good performance and and except for one trial where we got lower and worst results, when *Sigmoid Trick* is selected, in the remaning cases have gotten remarkable results. More precisely we can say what follows:\n",
    "\n",
    "- speaking about __Linear kernel Pca based Svm Classifier__, when adoping the default threshold of *.5* for classification purposes we have a model that reaches an accuracy of *71%* at test time against an accuracy of *89%* at train step, while the Auc score reaches a value of *84%* with a Roc Curve that shows a behavior for which the model for a wide range of possible thresholds lower than .5 seems to improve over *TPR* quicklier than *FPR*, unitl the Roc Curve turns so that also *FPR* increases linearly with *TPR* metric. The resulting model shows both high precision and high recall when considering the performance of such metrics related to class 1, while reflecting upon class 0 performance metrics we notice as for other previous models that generally the several classifiers and estimators tend to show high recall, so correctly predict labels for true class 0 examples, but relatively low precision due to the fact that since the dataset is unbalanced on average one third of the true class 1 samples are misclassified and such a number is comparable with the amount of correctly classified true class 0 examples. However, such a model do not overfit too much against train set, since we can see that on test set has obtained an accuracy of just 10 percent point less than the train accuracy score. Finally, looking at the Heatmap related to the validation accuracy score when combining C-vs-Gamma parameters, what we gain from such analysis is that the model when tuned mixing also others hyper-params still exploit the same values of *C and Gamma parameters* for indentifying the best tuned model.\n",
    "\n",
    "- observing __Polynomial kernel Pca based Svm Estimator__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *74%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *50%*. This model as we can clearly understand is a trial in which the final hyper-params configuration leads to a higly overfitting model to the training set, in fact we lost nearly 30% of accuracy comparing train versu test accuracy scores. Moreover, the Auc score is equals to the random classifier refernce model, this means that no matter which thresholds we set, such a classifier will predict still with a probability of *50%* the righ label of data points to be classified. In other words we should avoid such a model and we should not include in any machine learning system for inference tasks. However, such a model reveals to be really precise when predicting class labels for class 1 examples, instead generally shows poor performance for recall and precision of class 1 and class 0 respectively, where predicts just a bit more than half samples from class 1 and when predicting class 0 we know the classifier is very uncertain, even if the recall of class 0 is very high, since no samples where misclassified. Finally, looking at the *heatmap that shows C-vs-Gamma behaviors*, we notably understand that the model exploits the values for such parameters that are ot so far from those identifyed from the more complete grid-search analysis computed accounting also other hyper-params values.\n",
    "\n",
    "- looking at __Rbf kernel Pca based Svm Classifier__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *59%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *62%*...\n",
    "\n",
    "- looking at __Cosine kernel Pca based Svm Classifier__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *59%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *62%*...\n",
    "\n",
    "- finally, referring to __Sigmoid kernel Pca based Svm Model__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *65%* at test time against an accuracy of *92%* at train step, while the Auc score reaches a value of *72%*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_widget_list_df([df_gs, df_auc_gs]) #print(df_gs); print(df_auc_gs)\n",
    "show_table_summary_grid_search(df_gs, df_auc_gs, df_pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table dispalyed just above that shows the details about the selected values for hyper-parameters specified during grid search, in the different situations accordingly to the fixed kernel-trick for kernel Pca unsupervised method we can state that, referring to the first two columns of *Train and Test Accuracy*, we can recognize which trials lead to more overfit results such as for *Linear, Polynomial, and Sigmoid Tricks* or less overfit solution such as in the case of *Cosine, Trick*. Speaking about the hyper-parameters, we can say what follows:\n",
    "\n",
    "- referring to the svm's __Gamma Parameter__, which we recall that corresponds normally to *Kernel Coefficient* for *rbf, poly and sigmoid reprojection strategies*, we also remind that intuitively, the gamma parameter defines how far the influence of a single training example reaches, with *low values* meaning *far* and *high values* meaning *close*. The gamma parameters can be seen as the inverse of the radius of influence of samples selected by the model as support vectors.\n",
    "\n",
    "- looking at svm's __C Parameter__, and suggesting that such a parameter briefly trades off correct classification of training examples against maximization of the decision function’s margin. For larger values of C, a smaller margin will be accepted if the decision function is better at classifying all training points correctly. So that, a lower C will encourage a larger margin, therefore a simpler decision function, at the cost of training accuracy. In other words *C* behaves as a regularization parameter in the SVM.\n",
    "\n",
    "- instead, when speaking about svm's __Degree Parameter__, which is degree of the *polynomial kernel function (poly)*, therefore ignored by all other kernels, we are talking about a niche hyper-params that usually taking larger values implyes attempting to fit models that tends to lower bias values and higher variance, instead taking into account lower polynomial order my bit on higher bias and lower variance models. In other words, in one case we attempt to explain with a more fancy model a problem that might not follow a linear and smooth trend, vice versa, in the second case we prefer to represent our model by means of a lower capaciy classifier, easier to explain and understand.\n",
    "- considering svm's __coef0 Parameter__, which is Independent term in kernel function. It is only significant in svm kernel tricks' *poly and sigmoid* ...\n",
    "\n",
    "- the svm's __Shrinking Parameter__, from research field we found that if the number of iterations is large, then shrinking can shorten the training time..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines References\n",
    "- (Discriminative Model: SVM) https://scikit-learn.org/stable/modules/svm.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
