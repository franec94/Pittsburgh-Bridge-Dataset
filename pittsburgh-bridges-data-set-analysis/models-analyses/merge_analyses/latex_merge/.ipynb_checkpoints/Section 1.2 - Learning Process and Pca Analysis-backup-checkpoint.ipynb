{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Process <a class=\"anchor\" id=\"learning-models\"></a>\n",
    "\n",
    "Here in this section we are going to partly describy and in the remaining to test and evaluate performance of various machine learning models that we selected and adopted to built up learning models for accomplishing supervised machine learning tasks related to classification problems. More precisely, we focused on binary classification problemsa, since the target variables, that is T-OR-D feature, amongst the 12 features from which the dataset is made of, and from which the more or less hundered of records are described, is a binary categorical feature which can assume the two values represented by labels DECK and THROUGH describing in two distinct manner a property about each bridge within the dataset, that property refers to the system used for constructing the bridge surface, that is commonly called deck, for let veicheles or trains or whatevert to cross the rivers that are three distinct rivers: A, M, O. Where A stands for Allegheny river, while M stands for Monongahela river and lastly O stands for Ohio river.\n",
    "\n",
    "Before describing the fine tuning process applied to the different models accordingly to their own major properties, characteristics and features, we have decided and established to test each model performance by looking at how well all of them is going just exploiting the default setting and runnng cross-validation protocol, in other word also referred to as policy, to check the accuracy level, as ana instance and some other metrics.\n",
    "\n",
    "To be more detailed we follow the common machine leaning working flow, that requires to split the data set, after having preprocessed it properly and in a suitable manner to meet mahcine learning models needs, into subsets commonly referred  to as training set and test set. Where, the former is exploited to build up a inference model and the latter is used to check model's performance as weel as behavior up on held out sample of istances never seen before and so those examples that the learning model wasn't provided with to learn weights and select right hyper-params to plug in back into model at the end of training procedure, in order to later test the model with the found weights as well as hyper-parameters and if it meet our requirements in terms of performance values reached at test time, then it will be ready for deployments.\n",
    "\n",
    "<img src=\"images/machine_learning_workflow_2.png\" alt=\"machine learning workflow\" style=\"width: 400px;\"/>\n",
    "\\begin{figure}[h]\n",
    "\\caption{Machine Learning Workflow Example}\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{images/machine_learning_workflow_2.png}\n",
    "\\end{figure}\n",
    "\n",
    "As we can see from the image just above what we have to do, following the purposed machine learning scheme or work flow are the subsequent steps:\n",
    "\n",
    "__(1) Split Dataset into Train and Test sets__:\n",
    "After having done with preprocessing phase we separate or divide the dataset into training set and test set, where usually the training set is bigger than the test set, and in some cases  the test set, for recording some kind of performance measures, can be made of just a single instance against which the model will be tested and checked.\n",
    "\n",
    "__(2) Split Train set into smaller Train set and a Validation set__:\n",
    "Once  have made the first split then we hold out a part for later checking the test set and we focus on training set. In fact once we have selected a machine learning model amongst those we want to adopt to fit a model and compare the results obtained we try to indentify the best fit parameters to setting the model with them. To reach such a goal we can adopt different aproaches to split further the training set into a validation set and a smaller train set in order to emulate before doing test phase a possible beahvior of the trainng model once we think it is ready for the following phase that is test step.\n",
    "There are several procedure for establishing how to divide training set into two new subsets, that are validation and a little bit smaller train set, where all of them are also connected or reffered to a learning algorithm called cross-validation which consists roughly speaking into testing the model against a smaller portion of training set in order to record and measure the performance of a model before saying we are ready to proceed with test phase. Among the existing Cross-Validation Procedure we adopt and so describe briefly the following:\n",
    "\n",
    "- **K-fold Cross-Validation**: which is a cross validation protocol exploited so that we split the training set into K-folds, in other words K-subsets all of the same size, a part eventually the last one which will be the remainder set of samples. One at a time the K-fold are left out and the model is firstly trained against K-1 subsets(folds) and then test against the left out fold for recording the performanc. At the end of the k-times we have trained the model we have recorded the performance measures and we so can average the results and understand how model in average is well doing. In oder words we can either assume the mean value as the driving value to assume the model as satisfying our constraint on performance measures or adopt the best result amongst the k-trains as the settings for hyper-params to adopt. This procedure is feasible and suitable if we do not carea botu the fact that, in cases of classification tasks, the categories might be balanced or not in terms of number of instances, as well as, it can be adopted also if we want to show a learning curve and some other performance graphics or schemes as Confusion matrtices and ROC or recall-precision curves. Latly usual values for K are: 5, 10, 20.\n",
    "\n",
    "<img src=\"images/machine_learning_workflow/cross_validation.png\" alt=\"cross validation machine learning workflow\" style=\"width: 400px;\"/>\n",
    "\\begin{figure}[h]\n",
    "\\caption{Cross Validation Example}\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{images/machine_learning_workflow/cross_validation.png}\n",
    "\\end{figure}\n",
    "\n",
    "- **Leave One Out Cross-Validation**: It is a special case of the K-fold Cross-Validation. In fact, instead of adopting the usual values as 5, 10, 20, for K as the number of possible subsets, we establish to identify each single instance as a possible fold. It's clear that this algorithm requires more time to be completed and does not allow to show up the graphics cited just above, since we do not enough data for confusion matrix and ROC or recall-precision curves.\n",
    "\n",
    "- **Stratified Cross-Validation**: It is a good compromise when the dataset is still large enough to be exploited for training purposes but we decide to split into K-folds the traning set such that each subset have the same proportion of samples coming from the differn classes. This operation reveals to be necessary or even mandatory when we detect that the dataset does not show the same number of samples, more or less, for each class, in other word when the dataset is unbalanced for a given attribute that is the target attribute. This means that, while trying to mitigate the issue about the unbalanced dataset we thunk as well as hope that this management let the model to fit a model which will not be affected heavily just from the most numerous class, bus still learn how to classify the samples coming from the other less numerous classes, without too mcuh misclassifying errors. As usual also with Stratified Cross-Validation we are able to show same graphics as with plain K-fold Cross-Validation, the difference is that the folds are not randomly sampled from the original training set, but yet are sampled in the same proportion per each class in order to have the same number of samples for each class inside each fold.\n",
    "\n",
    "We try out all of the three described cross-validation techniques to measure how weel default settings for different models are doing to gaina a base line against wich compare later results coming from fine tuning process carried out exploiting grid search technique for selecting the best combinatin of purposed values for each candidate machine learning technique.\n",
    "\n",
    "__(3) Grid Search approach__: is the technique adopted when taking into account one at a time all the machine learning algorithm for selecting the best set of hyper-parameters. It consists into defining for each model a grid of possible values for different hyper-parameters that in some sense represent our degree of freedom referred to some of the properties that characterize all the different models. The grid of values might be real number ranging within a more or less interval, or a string value used to trigger a certain feature of a model combined with other related aspects of the machine learning algorithm of the given model. We recall that the standard grid search will proceed until all possible combination of provided values have been test and training with such settings have been carried out. Opposite to classic grid search it is another technique called Random Grid Search, which implies instead to let a model to choose or sample randomly the hyper-parameters within the ranges or intervals related to each hyper-params. The latter technique can be potentially less expensive since we test a reduced number of combination but might be sub-optimal even if the results can be still acceptable and meaningful.\n",
    "\n",
    "<img src=\"images/machine_learning_workflow/grid_search.png\" alt=\"grid search\" style=\"width: 400px;\"/>\n",
    "\\begin{figure}[h]\n",
    "\\caption{Grid Search Example}\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{images/machine_learning_workflow/grid_search.png}\n",
    "\\end{figure}\n",
    "\n",
    "__(4) Metrics and model performance's evaluation tools__: before deploying we have to test our model against a test set, that is a subset created or obtained from overall dataset which was, after preprocessing phase that turns feature values somehow into numbers, devided into two distinct sets generally of different size. The test set evaluation implies exploiting some metrics such as Accuracy, but yet there exist several others that partly are derived from confusion matrix evaluation tool, such as Precsio, Recal F1-score, and the like.\n",
    "\n",
    "So what we understand is that we can make use of a bouch of metrics but rather than using directly those metrics we can explore model's performance by means of more useful tools such as Confusion Matrix, Roc curve in order to better understanding model's behavior when fed with unobserved new samples, as well as, how to set a threshold for determing when a target variable will suggest us that the classified sample belong to one class or the other. So, here we briefly describe which instruments we exploit to measure model's peformance starting from confusion matrix and moving ahead toward ROC curve.\n",
    "\n",
    "**Confusion Matrix**: in statistics a confusion matrix it's a grid or matrix of numbers and in the simplest scenario, correspoding to a binary classification task, it aims at showing how well a model was going when applied onto unknow or preiviously unseen data points and  samples, in the following manner. Arbitrary we establish that along the rows we have the fraction of samples that the model has calssified or has assigned to agiven class, and so the rows account for *Predicted valuers*. Vice versa, along the columns we have the total number of samples for each class that all together resemble the so called *Actual Values* as we illustrate in the picture just below:\n",
    "\n",
    "<img src=\"images/machine_learning_workflow/confusion_matrix.png\" alt=\"confusion matrix example\" style=\"width: 400px;\"/>\n",
    "\\begin{figure}[h]\n",
    "\\caption{Confusion Matrix Example}\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{images/machine_learning_workflow/confusion_matrix.png}\n",
    "\\end{figure}\n",
    "\n",
    "So, such a table of numbers allows us to measure the fraction of correctly classified examples belonging to the Positive class, also referred to as *True Positive(TP)* or to the Negative class, also named *True Negative(TN)* and at the same time we can derive also the fractions of wrongly classified Positive samples and Negative ones, respectively knwon as *False Positve(FP)* and *False Negative (FN)*. It is clear that looking at the diagonal matrix we can undestand that the larger the value along the diagonal the better the model was able to correctly identify the samples accordingly to their own actual class.\n",
    "From the four statistics measure depiceted above, that are TP,TN, FP, and FN, by the time have been dirved other useful metrics that can be exploited when the most used and well knwon measurement performance of accuracy is not enough due to the fact as an instance we wanto to analyze deeper our model behavior toward optimization of some goal or because we have to deal with dataset which are not balanced through classes to be predicted and so we wanto some other metrics to ensure the goodness of our solutions.\n",
    "\n",
    "**Roc Curve**: for the reason cited some lines earlier by exploiting the four basis metrics have been developed other useful tools for ensuring the goodness of a solution, among those tools we decide to adopt the following one, knwon as Roc Curve. It is a curve, whose acronim stands for Receiver Operating Curve, largely employed in the field of *Decison Theory* and aims at finding, siggesting or showing how moedl's performance varywhen we are going to set different thresholds for a simple scenario in which we are going to solve a binary classification problem. Such a curve require to show on the x-axis the fraction of samples corresponding to *False Positve Rate (FPR)* at a different values of the model's hyper-params corresponding to threshold set for classifying items at inference time, as well as on the x-axis the fraction of samples corresponding to *True Positive Rate(TRP)* in order to plot a curve that originates at coordinates (0,0) and terminates at coordinates (1,1), varying in the middle accordingly to the pairs of values recorded at a given threshold for (FPR,TPR) pair. We are also reporting two driving curves that are respectvely the curve related to the *Random Classifier* which corresponds to a classifier that for each instance is just randomly selecting the predicted class, and the *Perfetct Classifier* that always correctly classifyes all the samples. Our goal by analysing such a graphics is to identify the threshold value such that we are near the points on the curve that are not so far from the upper-left corner so that to maximise the TPR as well as minimizing the FPR. Another useful quantity related to Roc Curve is represented by the so called amount Area Under the Curve (AUC) that suggests us how much area under the rco curve whas accounted while varying the threshold for classiffying the test samples, in particular the higher the value the better the classifier is doing varying the threshlds. Lastly we notice that the Random Sample accounts for an AUC equls to 0.5 %, while the Perfect Classifier for 1.0 % so we aim at obtaing a value for AUC that is in at least in the between but that approaches the unity. Here, below is presented an example of Roc Curve example:\n",
    "\n",
    "<img src=\"images/machine_learning_workflow/roc_curve.jpg\" alt=\"roc curve example\" style=\"width: 400px;\"/>\n",
    "\\begin{figure}[h]\n",
    "\\caption{Roc Curve Example}\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{images/machine_learning_workflow/roc_curve.jpg}\n",
    "\\end{figure}\n",
    "\n",
    "Lastly, before moving ahead with describing Machine Learning Models we provide a brief list of other useful metrics that can be exploited if necessary during our analyses:\n",
    "\n",
    "<img src=\"images/machine_learning_workflow/summary_statistics.gif\" alt=\"summary statistics\" style=\"width: 400px;\"/>\n",
    "\\begin{figure}[h]\n",
    "\\caption{Roc Curve Example}\n",
    "\\centering\n",
    "\\includegraphics[width=0.5\\textwidth]{images/machine_learning_workflow/summary_statistics.jpg}\n",
    "\\end{figure}\n",
    "\n",
    "**Learning Curve**:\n",
    "learning curves constitute a great tool to diagnose bias and variance in any supervised learning algorithm. It comes in handt when we have to face against the so called **Bias-Variance Trade-Off**. In order to explain what that trade-off implies we have to say briefly what follows:\n",
    "- In supervised learning, we assume there's a real relationship between feature(s) and target and estimate this unknown relationship with a model. Provided the assumption is true, there really is a model, which we'll call $f$, which describes perfectly the relationship between features and target.\n",
    "- In practice, $f$ is almost always completely unknown, and we try to estimate it with a model $\\hat{f}$. We use a certain training set and get a certain $\\hat{f}$. If we use a different training set, we are very likely to get a different $\\hat{f}$. As we keep changing training sets, we get different outputs for $\\hat{f}$. The amount by which $\\hat{f}$ varies as we change training sets is called **variance**.\n",
    "- While, for most real-life scenarios, however, the true relationship between features and target is complicated and far from linear. Simplifying assumptions give **bias** to a model. The more erroneous the assumptions with respect to the true relationship, the higher the bias, and vice-versa.\n",
    "- In practice, however, we need to accept a trade-off. We can’t have both low bias and low variance, so we want to aim for something in the middle, knowing that:\n",
    "\\begin{equation}\n",
    "\\begin{cases}\n",
    "Y = f(X) + \\quad irreducible \\quad error \\\\\n",
    "f(X) = \\hat{f(X)} + \\quad reducible \\quad error \\\\\n",
    "Y = \\hat{f(X)} + \\quad reducible \\quad error + \\quad irreducible \\quad error \\\\\n",
    " \\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "<table><tr>\n",
    "    <td><img src=\"images/machine_learning_workflow/biasvariance.png\" alt=\"summary statistics\" style=\"width: 400px;\"/> </td>\n",
    "    <td> <img src=\"images/machine_learning_workflow/irr_error.png\" alt=\"Irreducable Error\" style=\"width: 400px;\"/> </td>\n",
    "</tr></table>\n",
    "\\begin{figure}[H]\n",
    "\t\\centering\n",
    "\t\\begin{subfigure}{.3\\textwidth}\n",
    "\t\t\\includegraphics[width=\\textwidth]{images/machine_learning_workflow/biasvariance.png}\n",
    "\t\t\\caption{Atom 1}\n",
    "\t\\end{subfigure}\n",
    "%%%%%%%%%%%%%%\n",
    "\t\\begin{subfigure}{.3\\textwidth}\n",
    "\t\t\\includegraphics[width=\\textwidth]{images/machine_learning_workflow/irr_error.png}\n",
    "\t\t\\caption{Atom 2}\n",
    "\t\\end{subfigure}\n",
    "%%%%%%%%%%%%%%\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "<table><tr>\n",
    "    <td> <img src=\"images/machine_learning_workflow/add_data.png\" alt=\"add data\" style=\"width: 400px;\"/></td>\n",
    "    <td> <img src=\"images/machine_learning_workflow/low_high_var.png\" alt=\"low high var\" style=\"width: 400px;\"/></td>\n",
    "</tr></table>\n",
    "\\begin{figure}[H]\n",
    "\t\\centering\n",
    "\t\\begin{subfigure}{.3\\textwidth}\n",
    "\t\t\\includegraphics[width=\\textwidth]{images/machine_learning_workflow/add_data.png}\n",
    "\t\t\\caption{Atom 1}\n",
    "\t\\end{subfigure}\n",
    "%%%%%%%%%%%%%%\n",
    "\t\\begin{subfigure}{.3\\textwidth}\n",
    "\t\t\\includegraphics[width=\\textwidth]{images/machine_learning_workflow/low_high_var.png}\n",
    "\t\t\\caption{Atom 2}\n",
    "\t\\end{subfigure}\n",
    "%%%%%%%%%%%%%%\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "**P-Value Analysis**: Here, in the following we are going to describe shortly what is and how we have exploited the statistical tool for assessing model performance from a particular point of view represented by the so called *P-value Analysis*. The p-value is widely used in *statistical hypothesis testing*, specifically in *null hypothesis significance testing*. In this method, as part of experimental design, before performing the experiment, one first chooses a model (the null hypothesis) and a threshold value for p, called the *significance level* of the test, traditionally *5% or 1%* and denoted as $\\alpha$. If the p-value is less than the chosen significance level ($\\alpha$), that suggests that the observed data is sufficiently inconsistent with the null hypothesis and that the null hypothesis may be rejected. However, that does not prove that the tested hypothesis is true. When the p-value is calculated correctly, this test guarantees that the type I error rate is at most $\\alpha$. For typical analysis, using the standard $\\alpha = 0.05$  cutoff, the null hypothesis is rejected when $\\rho < .05$ and not rejected when $\\rho > .05$. The p-value does not, in itself, support reasoning about the probabilities of hypotheses but is only a tool for deciding whether to reject the null hypothesis.\n",
    "\n",
    "__(5)Deployment__: The last step for building a machine learning system comprise the deployment of one or more machine learning trained models that fit and satisfy constraints that was intially fixed before doing any kind of analysis. The main goal of deployemnt is to employ such statistics models for predicting, in other words making inference, against new data, unknown obeservatons for which we do not knwo their target class values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pricipal Component Analysis\n",
    "\n",
    "After having investigate the data points inside the dataset, I move one to another section of my report where I decide to explore examples that made up the entire dataset using a particular technique in the field of statistical analysis that corresponds, precisely, to so called Principal Component Analysis. In fact, the major objective of this section is understand whether it is possible to transform, by means of some kind of linear transformation given by a mathematical calculation, the original data examples into reprojected representation that allows me to retrieve most useful information to be later exploited at training time. So, lets dive a bit whitin what is and which are main concepts, pros and cons about Principal Component Analysis.\n",
    "\n",
    "Firstly, we know that **Principal Component Analysis**, more shortly PCA, is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called *principal components*. This transformation is defined in such a way that:\n",
    "- the first principal component has the largest possible variance (that is, accounts for as much of the variability in the data as possible),\n",
    "- and each succeeding component in turn has the highest variance possible under the constraint that it is orthogonal to the preceding components.\n",
    "\n",
    "The resulting vectors, each being a linear combination of the variables and containing n observations, are an uncorrelated orthogonal basis set. PCA is sensitive to the relative scaling of the original variables.\n",
    "\n",
    "PCA is mostly used as a tool in *exploratory data analysis* and for making predictive models, for that reasons I used such a technique here, before going through the different learning technique for producing my models.\n",
    "\n",
    "#### Several Different Implementation\n",
    "\n",
    "From the theory and the filed of research in statistics, we know that out there, there are several different implementation and way of computing principal component analysis, and each adopted technique has different performance as well as numerical stability. The three major derivations are:\n",
    "- PCA by means of an iterative based procedure of extraing pricipal components one after the other selecting each time the one that account for the most of variance along its own axis, within the remainig subspace to be derived.\n",
    "- The second possible way of performing PCA is done via calculation of *Covariance Matrix* applied to attributes, that are our independent predictive variables, used to represent data points.\n",
    "- Lastly, it is used the technique known as *Singular Valued Decomposition* applied to the overall data points within our dataset.\n",
    "\n",
    "Reading scikit-learn documentation, I discovered that PCA's derivation uses the *LAPACK implementation* of the *full SVD* or a *randomized truncated SVD* by the method of *Halko et al. 2009*, depending on the shape of the input data and the number of components to extract. Therefore I will descrive mainly that way of deriving the method with respect to the others that, instead, will be described more briefly and roughly.\n",
    "\n",
    "#### PCA's Iterative based Method\n",
    "Going in order, as depicted briefly above, I start describing PCA obtained by means of iterative based procedure to extract one at a time a new principal componet explointing the data points at hand.\n",
    "\n",
    "We begin, recalling that, PCA is defined as an orthogonal linear transformation that transforms the data to a new coordinate system such that the greatest variance by some scalar projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "\n",
    "We suppose to deal with a data matrix X, with column-wise zero empirical mean, where each of the n rows represents a different repetition of the experiment, and each of the p columns gives a particular kind of feature.\n",
    "\n",
    "From a math poitn of view, the transformation is defined by a set of p-dimensional vectors of weights or coefficients $\\mathbf {w} _{(k)}=(w_{1},\\dots ,w_{p})_{(k)}$ that map each row vector $\\mathbf{x}_{(i)}$ of X to a new vector of principal component scores ${\\displaystyle \\mathbf {t} _{(i)}=(t_{1},\\dots ,t_{l})_{(i)}}$, given by: ${\\displaystyle {t_{k}}_{(i)}=\\mathbf {x} _{(i)}\\cdot \\mathbf {w} _{(k)}\\qquad \\mathrm {for} \\qquad i=1,\\dots ,n\\qquad k=1,\\dots ,l}$.\n",
    "\n",
    "In this way all the individual variables ${\\displaystyle t_{1},\\dots ,t_{l}}$ of t considered over the data set successively inherit the maximum possible variance from X, with each coefficient vector w constrained to be a unit vector.\n",
    "\n",
    "More precisely, the first component In order to maximize variance has to satisfy the following expression:\n",
    "\n",
    "${\\displaystyle \\mathbf {w} _{(1)}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\left\\{\\sum _{i}\\left(t_{1}\\right)_{(i)}^{2}\\right\\}={\\underset {\\Vert \\mathbf {w} \\Vert =1}{\\operatorname {\\arg \\,max} }}\\,\\left\\{\\sum _{i}\\left(\\mathbf {x} _{(i)}\\cdot \\mathbf {w} \\right)^{2}\\right\\}}$\n",
    "\n",
    "So, with $w_{1}$ found, the first principal component of a data vector $x_{1}$ can then be given as a score $t_{1(i)} = x_{1} ⋅ w_{1}$ in the transformed co-ordinates, or as the corresponding vector in the original variables, $(x_{1} ⋅ w_{1})w_{1}$.\n",
    "\n",
    "The others remainig components are computed as folloes. The kth component can be found by subtracting the first k − 1 principal components from X, as in the following expression:\n",
    "\n",
    "- ${\\displaystyle \\mathbf {\\hat {X}} _{k}=\\mathbf {X} -\\sum _{s=1}^{k-1}\\mathbf {X} \\mathbf {w} _{(s)}\\mathbf {w} _{(s)}^{\\rm {T}}}$\n",
    "\n",
    "- and then finding the weight vector which extracts the maximum variance from this new data matrix  ${\\mathbf {w}}_{{(k)}}={\\underset {\\Vert {\\mathbf {w}}\\Vert =1}{\\operatorname {arg\\,max}}}\\left\\{\\Vert {\\mathbf {{\\hat {X}}}}_{{k}}{\\mathbf {w}}\\Vert ^{2}\\right\\}={\\operatorname {\\arg \\,max}}\\,\\left\\{{\\tfrac {{\\mathbf {w}}^{T}{\\mathbf {{\\hat {X}}}}_{{k}}^{T}{\\mathbf {{\\hat {X}}}}_{{k}}{\\mathbf {w}}}{{\\mathbf {w}}^{T}{\\mathbf {w}}}}\\right\\}$\n",
    "\n",
    "It turns out that:\n",
    "- from the formulas depicted above me get the remaining eigenvectors of $X^{T}X$, with the maximum values for the quantity in brackets given by their corresponding eigenvalues. Thus the weight vectors are eigenvectors of $X^{T}X$.\n",
    "- The kth principal component of a data vector $x_(i)$ can therefore be given as a score $t_{k(i)} = x_{(i)} ⋅ w_(k)$ in the transformed co-ordinates, or as the corresponding vector in the space of the original variables, $(x_{(i)} ⋅ w_{(k)}) w_{(k)}$, where $w_{(k)}$ is the kth eigenvector of $X^{T}X$.\n",
    "- The full principal components decomposition of X can therefore be given as: ${\\displaystyle \\mathbf {T} =\\mathbf {X} \\mathbf {W}}$, where W is a p-by-p matrix of weights whose columns are the eigenvectors of $X^{T}X$.\n",
    "\n",
    "#### Covariance Matrix for PCA analysis\n",
    "\n",
    "PCA made from covarian matrix computation requires the calculation of sample covariance matrix of the dataset as follows: $\\mathbf{Q} \\propto \\mathbf{X}^T \\mathbf{X} = \\mathbf{W} \\mathbf{\\Lambda} \\mathbf{W}^T$.\n",
    "\n",
    "The empirical covariance matrix between the principal components becomes ${\\displaystyle \\mathbf {W} ^{T}\\mathbf {Q} \\mathbf {W} \\propto \\mathbf {W} ^{T}\\mathbf {W} \\,\\mathbf {\\Lambda } \\,\\mathbf {W} ^{T}\\mathbf {W} =\\mathbf {\\Lambda } }$.\n",
    "\n",
    "\n",
    "#### Singular Value Decomposition for PCA analysis\n",
    "\n",
    "Finally, the principal components transformation can also be associated with another matrix factorization, the singular value decomposition (SVD) of X, ${\\displaystyle \\mathbf {X} =\\mathbf {U} \\mathbf {\\Sigma } \\mathbf {W} ^{T}}$, where more precisely:\n",
    "- Σ is an n-by-p rectangular diagonal matrix of positive numbers $σ_{(k)}$, called the singular values of X;\n",
    "- instead U is an n-by-n matrix, the columns of which are orthogonal unit vectors of length n called the left singular vectors of X;\n",
    "- Then, W is a p-by-p whose columns are orthogonal unit vectors of length p and called the right singular vectors of X.\n",
    "\n",
    "factorizingn the matrix ${X^{T}X}$, it can be written  as:\n",
    "\n",
    "${\\begin{aligned}\\mathbf {X} ^{T}\\mathbf {X} &=\\mathbf {W} \\mathbf {\\Sigma } ^{T}\\mathbf {U} ^{T}\\mathbf {U} \\mathbf {\\Sigma } \\mathbf {W} ^{T}\\\\&=\\mathbf {W} \\mathbf {\\Sigma } ^{T}\\mathbf {\\Sigma } \\mathbf {W} ^{T}\\\\&=\\mathbf {W} \\mathbf {\\hat {\\Sigma }} ^{2}\\mathbf {W} ^{T}\\end{aligned}}$\n",
    "\n",
    "Where we recall that ${\\displaystyle \\mathbf {\\hat {\\Sigma }} }$ is the square diagonal matrix with the singular values of X and the excess zeros chopped off that satisfies ${\\displaystyle \\mathbf {{\\hat {\\Sigma }}^{2}} =\\mathbf {\\Sigma } ^{T}\\mathbf {\\Sigma } } {\\displaystyle \\mathbf {{\\hat {\\Sigma }}^{2}} =\\mathbf {\\Sigma } ^{T}\\mathbf {\\Sigma } }$. Comparison with the eigenvector factorization of $X^{T}X$ establishes that the right singular vectors W of X are equivalent to the eigenvectors of $X^{T}X$ , while the singular values $σ_{(k)}$ of X are equal to the square-root of the eigenvalues $λ_{(k)}$ of $X^{T}X$ . \n",
    "\n",
    "At this point we understand that using the singular value decomposition the score matrix T can be written as:\n",
    "\n",
    "${\\begin{aligned} \\mathbf{T} & = \\mathbf{X} \\mathbf{W} \\\\ & = \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{W}^T \\mathbf{W} \\\\ & = \\mathbf{U}\\mathbf{\\Sigma} \\end{aligned}}$\n",
    "\n",
    "so each column of T is given by one of the left singular vectors of X multiplied by the corresponding singular value. This form is also the polar decomposition of T.\n",
    "\n",
    "Efficient algorithms exist to calculate the SVD, as in scikit-learn package, of X without having to form the matrix $X^{T}X$, so computing the SVD is now the standard way to calculate a principal components analysis from a data matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from utils.all_imports import *;\n",
    "%matplotlib inline\n",
    "# Set seed for notebook repeatability\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len('2,5,6,7,8,9,10'.split(','))\n",
    "principal_components = [pc for pc in '2,5,6,7,8,9,10'.split(',')]\n",
    "data = np.zeros(shape=(n, 7))\n",
    "\n",
    "data[0:2,0:2] = [[0, 1], [1, 0]]\n",
    "data[4:6,0+3] = [0, 1]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================== #\n",
    "# READ INPUT DATASET\n",
    "# =========================================================================== #\n",
    "dataset_path, dataset_name, column_names, TARGET_COL = get_dataset_location()\n",
    "estimators_list, estimators_names = get_estimators()\n",
    "\n",
    "dataset, feature_vs_values = load_brdiges_dataset(dataset_path, dataset_name)\n",
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION']\n",
    "# Array used for storing graphs\n",
    "plots_names = list(map(lambda xi: f\"{xi}_learning_curve.png\", estimators_names))\n",
    "pca_kernels_list = ['linear', 'poly', 'rbf', 'cosine', 'sigmoid']\n",
    "cv_list = list(range(10, 1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary about Target Variable {target_col}\n",
      "--------------------------------------------------\n",
      "2    57\n",
      "1    13\n",
      "Name: T-OR-D, dtype: int64\n",
      "shape features matrix X, after normalizing:  (70, 11)\n"
     ]
    }
   ],
   "source": [
    "# Make distinction between Target Variable and Predictors\n",
    "# --------------------------------------------------------------------------- #\n",
    "rescaledX, y, columns = prepare_data_for_train(dataset, target_col=TARGET_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.pairplot(dataset, hue=TARGET_COL, height=2.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative varation explained(percentage) up to given number of pcs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th># PCS</th>\n",
       "      <th>CumVarExp(%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>47.738342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>75.856460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>82.615768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>88.413903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.0</td>\n",
       "      <td>92.661938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>9.0</td>\n",
       "      <td>95.976841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.0</td>\n",
       "      <td>98.432807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   # PCS  CumVarExp(%)\n",
       "0    2.0     47.738342\n",
       "1    5.0     75.856460\n",
       "2    6.0     82.615768\n",
       "3    7.0     88.413903\n",
       "4    8.0     92.661938\n",
       "5    9.0     95.976841\n",
       "6   10.0     98.432807"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = show_table_pc_analysis(X=rescaledX, show_cum_exp_kpca=True); df.to_csv('cumulative_varation_explained.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Pros & Cons of PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Error Rate versus Training Sample Size\n",
    "\n",
    "Here, with the following set of graphics we aim at describing how the *Test Error Rate* changes across different *Training Set Size*, in  particular running the tests simply emploing the different estimators or classification algorithm with their default setting and running the training phase via cross-validation technique and adopint increasing training set size. Main goal with the illustration of such a bounch of graphics is to grasph a basic idea, that might be used to lead the training phase about how much the training set size affects the performance of a classifier at least with default settings of the hyper-parameter of the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try_comparing_various_online_solvers_by_kernel(X=rescaledX, y=y, kernels=None, heldout=[.75, .7, .65, .6, .55, .5, .45, .4, .35, .33, .3, .25, .2], rounds=20, n_classes=-1, n_components=9, estimators=estimators_list, cv=StratifiedKFold(2), verbose=0, show_fig=True, save_fig=False, stratified_flag=True, n_splits=3, random_state=42, gridshape=(3, 2), figsize=(15, 15), title=\"try comparing various online solvers by kernel\", fig_name=\"try_comparing_various_online_solvers_by_kernel.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main results we can get from the analysis we can carry out looking at the graphics just reported above is what follows:\n",
    "\n",
    "- the *GuassianNB Classifier* in four out of five trials, that are more precisely trial for which we have employed kernel-tricks such as Linear, Polynomial, Rbf, Sigmoid the resulting estimators follow a trend is characterized by increasing and decrising in test error rate with some large picks after we have used at least half of the samples from the dataset. Furthermore, the Polynomial based Estimator results to be the worst among those cosidered earlier, in fact near the half of the training set size the trend even becomes increasing going apart from the decreasing trend characterized from alternating local maxima and minima. Finally, when observing how such a classifier behaves when trained adopting Cosine trick we can refer that it follows a trend somewhat similar to the one followed also from other classifier expect for decision trees classifier.\n",
    "\n",
    "- the estimators based of *Decision Trees Classifier*, together with also *GuassianNB Classifiers*, are the two techniques amongst the others that were importantly affected by the training set size chosen for performing such analysis. In fact, even if in general is characterized from a decreasing trend, such a trend seems to be less stable than the others observed for the remaning classification techniques, in fact the trend is highly fluctuating with large picks when crossing and going towards larger training set sizes. The worst cases were when the classification technique was performed after having pre-preprocessed data examples by means of kernel-Pca based on Linear and Rbf tricks, because *Test Error Rate* turned to increasing after a given training set size which is larger than half of samples. While if we compare the generalization represented by Random Forest classifiers with respect to Decision Trees Classifier, it seems that the former across the different trials adopint also different kernel-trick for pre-processing the training examples follows a more stable decreasing trend for Test Error Rate versus training set size graphics, where just in two cases out of five which are trials where we have exploited Rbf and Sigmoid kernel-Pca tricks the random forest models seems to follwo in the second half of the Test Erro Rate curve a more fluctuating trend.\n",
    "\n",
    "- while, speaking about all the other learning algorithms, what we have noticed is that in general the several training phase executed for the models varying the training set size follows a decreasing trend, which means either that allowing the estimatores to be trained by means of larger training set they gain more insights about how can be characterized training examples, in order to better discriminate between the two classes, or that models overfit to much against the training set. However, all the models seem to start at the very beginning with a Test Error Rate somewhat yet not too large and keep reducing it across the different training set size. More precisely, we have reported that Knn Classifier performs better across the different attempts when the training set size was in the range between half and *70%* of the training set size. While SVMs, RandomForests, and Sgd classifiers seem to reach better performance when training examples in the training set reaches let to use a traning set size in the range between *70% and 80%* of the training set size. Lastly the logisti regression classifier seems to follow a decreasing trend for Test Error Rate which results to be the most conservative, with a reduced number of fluctuation which also have been a contained changes or fluctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = copy.deepcopy(rescaledX); n_components=9\n",
    "learning_curves_by_kernels(estimators_list[:], estimators_names[:], X, y, train_sizes=np.linspace(.1, 1.0, 10), n_components=n_components, pca_kernels_list=pca_kernels_list[0], verbose=0, by_pairs=True, savefigs=True, figs_dest=os.path.join('figures', 'learning_curve', f\"Pcs_{n_components}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can learn observig or looking at the bounch of graphics we have displayed just above is that not all of the different machine learning techiniques applied on dataset pre-processed by means of a precise kernel trick adopted for KernelPca show the same behavior, and even some plots suggest us that a machine learning algorithm may seem not suitable for such a small dataset for determining the well-known trend of a learning curve while exploiting a learning technique. To be more precise, we are going to speak roughly and briefly a little bit about each graphics, in the order as they have been shown here before, recalling that we have adopted up to 9 Principal components that account for nearly 95% of comulative explained variance related to the dataset at hand:\n",
    "\n",
    "- The first general observation or thought that we can explain is that for all the pictures the initial gap between the two curves is most of the time wide, large, in other words, important, more or less about 20% in percentage.\n",
    "\n",
    "- Speaking about both learning curve obtained from applying Gaussian Niave Bayes, before, and K-Nearest Neighbor, after, what we can say is that the curve related to Training Score is decreasing more or less of 10 point percentage, reaching 90% from the top, while the Cross-alidation curve seems to be constant for the first 6 trials and then to improve a little bit, but for the former model we can see increasing and descreasing that do not allow to fix a trend for measuring a gap between the two curves, instead for the latter model seems that the two curve converge at a certain point once the traingin set is more or less the overall dataset, which means we are going to exploit the most of information.\n",
    "\n",
    "- Instead, if we focus on Logistic Regression technique and SGD Classifier, we have observed that before a given numebr of trials in both picitures the two curves, that are Trainign and Cross-validation score curves, seem to follow an independent trend increasing and descrising independently, but then the two curves start to follow a decreasing trend so we can hipotize that after that precise trials we may have reached the desired training size.\n",
    "\n",
    "- The learning curve associated with SVC classifier suggests us that it seems to behave looking at both Traingin and Cross-Validation curve more or less as the previous graphics, but while the Training curve as decreases loosing accuracy score value such a curve tightenes the varaince, instead the Cross-validation curve keeps a wide varaince stretching it while increasing the training set size, meaaning that the models seem to be more unsure about the results.\n",
    "\n",
    "- The last two graphics are the most problematics and questionable ones, since while enalarging the training size we do not see either improvements or worsening referring to the Training curve, while the Cross-lvalidation curve seems at the very beginning to show a samew behavior as the Training curve, but then after a sequence of trials with increasing training size we get an improvement in terms of accuracy scores. But, since we normally expect to observe a learning curve where Training curve as well as Cross-validation curve respectively decreases and increases their performance at least at a given point where we hope the gap existing between them will be as small as possible wecan think that such those two graphics are nto explaining directly the real issues we face while dealing with Decsion Tree and Radom Forest Classifiers with such a small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "- Metrics and Diagnositc Tools:\n",
    "    - (__Confusion Matrix__) <font color='blue'>https://en.wikipedia.org/wiki/Confusion_matrix</font>\n",
    "    - (__F-1-Sccore, Accuracy, and Precision-Recall__) <font color='blue'>https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c</font>\n",
    "    - (__Roc Curve__) <font color='blue'>https://en.wikipedia.org/wiki/Receiver_operating_characteristic</font>\n",
    "    - (__P-value__) <font color='blue'>https://en.wikipedia.org/wiki/P-value</font>\n",
    "- Statistics:\n",
    "    - (__Correlation and dependence__) <font color='blue'>https://en.wikipedia.org/wiki/Correlation_and_dependence</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
