{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pittsburgh Bridges Data Set\n",
    "### Preprocessing stage: Heat map visualization after having cleaned the data\n",
    "\n",
    "- https://www.datacamp.com/community/tutorials/categorical-datas\n",
    "- http://cmdlinetips.com/\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
    "- https://chrisalbon.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "# === STANDARD IMPORTS ==== #\n",
    "print(__doc__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "# Matplotlib pyplot provides plotting API\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import chart_studio.plotly.plotly as py\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UTILS IMPORTS ==== #\n",
    "from utils.display_utils import display_heatmap\n",
    "from utils.display_utils import show_frequency_distribution_predictors\n",
    "from utils.display_utils import show_categorical_predictor_values\n",
    "from utils.display_utils import  show_cum_variance_vs_components\n",
    "\n",
    "from utils.preprocessing_utils import preprocess_categorical_variables\n",
    "from utils.preprocessing_utils import  preprocessing_data_rescaling\n",
    "\n",
    "from utils.training_utils import sgd_classifier_grid_search\n",
    "from utils.training_utils import plot_roc_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File /home/franec94/Documents/datasets/datasets_folders/pittsburgh-bridges-data-set/bridges.data.csv does not exist: '/home/franec94/Documents/datasets/datasets_folders/pittsburgh-bridges-data-set/bridges.data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e397a7b5c1fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# column_names = ['IDENTIF', 'RIVER', 'LOCATION', 'ERECTED', 'PURPOSE', 'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL', 'SPAN', 'REL-L', 'TYPE']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mcolumn_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'RIVER'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LOCATION'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ERECTED'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'PURPOSE'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LENGTH'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'LANES'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CLEAR-G'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'T-OR-D'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MATERIAL'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SPAN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'REL-L'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'TYPE'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}/{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\data-space\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data-space\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data-space\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data-space\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\data-space\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File /home/franec94/Documents/datasets/datasets_folders/pittsburgh-bridges-data-set/bridges.data.csv does not exist: '/home/franec94/Documents/datasets/datasets_folders/pittsburgh-bridges-data-set/bridges.data.csv'"
     ]
    }
   ],
   "source": [
    "# === READ INPUT DATASET ==== #\n",
    "dataset_path = '/home/franec94/Documents/datasets/datasets_folders/pittsburgh-bridges-data-set'\n",
    "dataset_name = 'bridges.data.csv'\n",
    "\n",
    "# column_names = ['IDENTIF', 'RIVER', 'LOCATION', 'ERECTED', 'PURPOSE', 'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL', 'SPAN', 'REL-L', 'TYPE']\n",
    "column_names = ['RIVER', 'LOCATION', 'ERECTED', 'PURPOSE', 'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL', 'SPAN', 'REL-L', 'TYPE']\n",
    "dataset = pd.read_csv('{}/{}'.format(dataset_path, dataset_name), names=column_names, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SHOW SOME STANDARD DATASET INFOS ==== #\n",
    "print('Dataset shape: {}'.format(dataset.shape))\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SHOWING FIRSTS N-ROWS AS THEY ARE STORED WITHIN DATASET === #\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INVESTIGATING DATASET IN ORDER TO DETECT NULL VALUES === #\n",
    "print('Before preprocessing dataset and handling null values')\n",
    "result = dataset.isnull().values.any()\n",
    "print('There are any null values ? Response: {}'.format(result))\n",
    "\n",
    "result = dataset.isnull().sum()\n",
    "print('Number of null values for each predictor:\\n{}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DISCOVERING VALUES WITHIN EACH PREDICTOR DOMAIN === #\n",
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION', 'LANES']\n",
    "# columns_2_avoid = None\n",
    "list_columns_2_fix = show_categorical_predictor_values(dataset, columns_2_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FIXING, UPDATING NULL VALUES CODED AS '?' SYMBOL  === #\n",
    "# === WITHIN EACH CATEGORICAL VARIABLE, IF DETECTED ANY === #\n",
    "print('Before', dataset.shape)\n",
    "for _, predictor in enumerate(list_columns_2_fix):\n",
    "    dataset = dataset[dataset[predictor] != '?']\n",
    "print('After', dataset.shape)\n",
    "\n",
    "_ = show_categorical_predictor_values(dataset, columns_2_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INTERMEDIATE RESULT FOUNDED === #\n",
    "preprocess_categorical_variables(dataset, columns_2_avoid)\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.boxplot('RIVER','TYPE',rot = 30,figsize=(5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[['LENGTH', 'SPAN', 'LANES']] = dataset[['LENGTH', 'SPAN', 'LANES']].replace(to_replace='?', value=None, method='bfill')\n",
    "# print(dataset['SPAN'].value_counts())\n",
    "# print(dataset['LENGTH'].value_counts())\n",
    "# print(dataset['LANES'].value_counts())\n",
    "\n",
    "print('Before', dataset.shape)\n",
    "columns_2_map = ['ERECTED', 'LANES']\n",
    "for _, predictor in enumerate(columns_2_map):\n",
    "    dataset = dataset[dataset[predictor] != '?']\n",
    "    dataset[predictor] = np.array(list(map(lambda x: int(x), dataset[predictor].values)))\n",
    "print('After', dataset.shape)\n",
    "print(dataset.info())\n",
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before', dataset.shape)\n",
    "columns_2_map = ['LOCATION', 'LANES', 'LENGTH']    \n",
    "for _, predictor in enumerate(columns_2_map):\n",
    "    dataset = dataset[dataset[predictor] != '?']\n",
    "    dataset[predictor] = np.array(list(map(lambda x: float(x), dataset[predictor].values)))\n",
    "print('After', dataset.shape)    \n",
    "print(dataset.info())\n",
    "print(dataset.head(5))\n",
    "\n",
    "# columns_2_avoid = None\n",
    "list_columns_2_fix = show_categorical_predictor_values(dataset, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dataset.isnull().values.any()\n",
    "# print('After handling null values\\nThere are any null values ? Response: {}'.format(result))\n",
    "\n",
    "result = dataset.isnull().sum()\n",
    "# print('Number of null values for each predictor:\\n{}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION']\n",
    "show_frequency_distribution_predictors(dataset, columns_2_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_result = dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_heatmap(corr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = dataset.columns\n",
    "target_col = 'T-OR-D'\n",
    "\n",
    "y = np.array(list(map(lambda x: 0 if x == 1 else 1, dataset[target_col].values)))\n",
    "print(dataset['T-OR-D'].value_counts())\n",
    "X = dataset.loc[:, dataset.columns != target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "scaler_methods = ['minmax', 'standard', 'norm']\n",
    "scaler_method = 'standard'\n",
    "rescaledX = preprocessing_data_rescaling(scaler_method, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_components = rescaledX.shape[1]\n",
    "pca = PCA(n_components=n_components)\n",
    "# pca = PCA(n_components=2)\n",
    "\n",
    "#X_pca = pca.fit_transform(X)\n",
    "pca = pca.fit(rescaledX)\n",
    "X_pca = pca.transform(rescaledX)\n",
    "    \n",
    "fig = show_cum_variance_vs_components(pca, n_components)\n",
    "\n",
    "py.sign_in('franec94', 'STMaADdoKsk66UekCPGa')\n",
    "py.iplot(fig, filename='selecting-principal-components {}'.format(scaler_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_methods = ['minmax', 'standard', 'norm']\n",
    "scaler_methods = []\n",
    "for _, scaler_method in enumerate(scaler_methods):\n",
    "    rescaledX = preprocessing_data_rescaling(scaler_method, X)\n",
    "    \n",
    "    n_components = rescaledX.shape[1]\n",
    "    pca = PCA(n_components=n_components)\n",
    "    # pca = PCA(n_components=2)\n",
    "\n",
    "    #X_pca = pca.fit_transform(X)\n",
    "    pca = pca.fit(rescaledX)\n",
    "    X_pca = pca.transform(rescaledX)\n",
    "    \n",
    "    show_cum_variance_vs_components(pca, n_components)\n",
    "    fig = show_cum_variance_vs_components(pca, n_components)\n",
    "\n",
    "    # py.sign_in('franec94', 'QbLNKpC0EZB0kol0aL2Z')\n",
    "    py.iplot(fig, filename='selecting-principal-components {}'.format(scaler_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components = [pc for pc in '2,5,6,7,8,9,10'.split(',')]\n",
    "for _, pc in enumerate(principal_components):\n",
    "    n_components = int(pc)\n",
    "    \n",
    "    cum_var_exp_up_to_n_pcs = np.cumsum(pca.explained_variance_ratio_)[n_components-1]\n",
    "    print(f\"Cumulative varation explained up to {n_components} pcs = {cum_var_exp_up_to_n_pcs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_crossval(rescaledX, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
