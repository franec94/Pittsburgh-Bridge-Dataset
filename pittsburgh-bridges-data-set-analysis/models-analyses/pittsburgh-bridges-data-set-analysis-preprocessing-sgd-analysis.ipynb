{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pittsburgh Bridges Data Set\n",
    "### SGD Analysis: SGD classifier trained by means of Grid Search approach\n",
    "\n",
    "- https://www.datacamp.com/community/tutorials/categorical-datas\n",
    "- http://cmdlinetips.com/\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py\n",
    "- https://chrisalbon.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STANDARD IMPORTS ==== #\n",
    "print(__doc__)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "# Matplotlib pyplot provides plotting API\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import chart_studio.plotly.plotly as py\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === UTILS IMPORTS ==== #\n",
    "from utils.display_utils import display_heatmap\n",
    "from utils.display_utils import show_frequency_distribution_predictors\n",
    "from utils.display_utils import show_categorical_predictor_values\n",
    "from utils.display_utils import  show_cum_variance_vs_components\n",
    "\n",
    "from utils.preprocessing_utils import preprocess_categorical_variables\n",
    "from utils.preprocessing_utils import  preprocessing_data_rescaling\n",
    "\n",
    "from utils.training_utils import sgd_classifier_grid_search\n",
    "from utils.training_utils import plot_roc_crossval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === READ INPUT DATASET ==== #\n",
    "dataset_path = '/home/franec94/Documents/datasets/datasets_folders/pittsburgh-bridges-data-set'\n",
    "dataset_name = 'bridges.data.csv'\n",
    "\n",
    "# column_names = ['IDENTIF', 'RIVER', 'LOCATION', 'ERECTED', 'PURPOSE', 'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL', 'SPAN', 'REL-L', 'TYPE']\n",
    "column_names = ['RIVER', 'LOCATION', 'ERECTED', 'PURPOSE', 'LENGTH', 'LANES', 'CLEAR-G', 'T-OR-D', 'MATERIAL', 'SPAN', 'REL-L', 'TYPE']\n",
    "dataset = pd.read_csv('{}/{}'.format(dataset_path, dataset_name), names=column_names, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SHOW SOME STANDARD DATASET INFOS ==== #\n",
    "print('Dataset shape: {}'.format(dataset.shape))\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SHOWING FIRSTS N-ROWS AS THEY ARE STORED WITHIN DATASET === #\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INVESTIGATING DATASET IN ORDER TO DETECT NULL VALUES === #\n",
    "print('Before preprocessing dataset and handling null values')\n",
    "result = dataset.isnull().values.any()\n",
    "print('There are any null values ? Response: {}'.format(result))\n",
    "\n",
    "result = dataset.isnull().sum()\n",
    "print('Number of null values for each predictor:\\n{}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DISCOVERING VALUES WITHIN EACH PREDICTOR DOMAIN === #\n",
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION', 'LANES']\n",
    "# columns_2_avoid = None\n",
    "list_columns_2_fix = show_categorical_predictor_values(dataset, columns_2_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FIXING, UPDATING NULL VALUES CODED AS '?' SYMBOL  === #\n",
    "# === WITHIN EACH CATEGORICAL VARIABLE, IF DETECTED ANY === #\n",
    "print('Before', dataset.shape)\n",
    "for _, predictor in enumerate(list_columns_2_fix):\n",
    "    dataset = dataset[dataset[predictor] != '?']\n",
    "print('After', dataset.shape)\n",
    "\n",
    "_ = show_categorical_predictor_values(dataset, columns_2_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === INTERMEDIATE RESULT FOUNDED === #\n",
    "preprocess_categorical_variables(dataset, columns_2_avoid)\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.boxplot('RIVER','TYPE',rot = 30,figsize=(5,6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset[['LENGTH', 'SPAN', 'LANES']] = dataset[['LENGTH', 'SPAN', 'LANES']].replace(to_replace='?', value=None, method='bfill')\n",
    "# print(dataset['SPAN'].value_counts())\n",
    "# print(dataset['LENGTH'].value_counts())\n",
    "# print(dataset['LANES'].value_counts())\n",
    "\n",
    "print('Before', dataset.shape)\n",
    "columns_2_map = ['ERECTED', 'LANES']\n",
    "for _, predictor in enumerate(columns_2_map):\n",
    "    dataset = dataset[dataset[predictor] != '?']\n",
    "    dataset[predictor] = np.array(list(map(lambda x: int(x), dataset[predictor].values)))\n",
    "print('After', dataset.shape)\n",
    "print(dataset.info())\n",
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before', dataset.shape)\n",
    "columns_2_map = ['LOCATION', 'LANES', 'LENGTH']    \n",
    "for _, predictor in enumerate(columns_2_map):\n",
    "    dataset = dataset[dataset[predictor] != '?']\n",
    "    dataset[predictor] = np.array(list(map(lambda x: float(x), dataset[predictor].values)))\n",
    "print('After', dataset.shape)    \n",
    "print(dataset.info())\n",
    "print(dataset.head(5))\n",
    "\n",
    "# columns_2_avoid = None\n",
    "list_columns_2_fix = show_categorical_predictor_values(dataset, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dataset.isnull().values.any()\n",
    "# print('After handling null values\\nThere are any null values ? Response: {}'.format(result))\n",
    "\n",
    "result = dataset.isnull().sum()\n",
    "# print('Number of null values for each predictor:\\n{}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION']\n",
    "show_frequency_distribution_predictors(dataset, columns_2_avoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_result = dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_heatmap(corr_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = dataset.columns\n",
    "target_col = 'T-OR-D'\n",
    "\n",
    "y = np.array(list(map(lambda x: 0 if x == 1 else 1, dataset[target_col].values)))\n",
    "print(dataset['T-OR-D'].value_counts())\n",
    "X = dataset.loc[:, dataset.columns != target_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardizing the features\n",
    "scaler_methods = ['minmax', 'standard', 'norm']\n",
    "scaler_method = 'standard'\n",
    "rescaledX = preprocessing_data_rescaling(scaler_method, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_components = rescaledX.shape[1]\n",
    "pca = PCA(n_components=n_components)\n",
    "# pca = PCA(n_components=2)\n",
    "\n",
    "#X_pca = pca.fit_transform(X)\n",
    "pca = pca.fit(rescaledX)\n",
    "X_pca = pca.transform(rescaledX)\n",
    "    \n",
    "fig = show_cum_variance_vs_components(pca, n_components)\n",
    "\n",
    "py.sign_in('franec94', 'QbLNKpC0EZB0kol0aL2Z')\n",
    "py.iplot(fig, filename='selecting-principal-components {}'.format(scaler_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler_methods = ['minmax', 'standard', 'norm']\n",
    "scaler_methods = []\n",
    "for _, scaler_method in enumerate(scaler_methods):\n",
    "    rescaledX = preprocessing_data_rescaling(scaler_method, X)\n",
    "    \n",
    "    n_components = rescaledX.shape[1]\n",
    "    pca = PCA(n_components=n_components)\n",
    "    # pca = PCA(n_components=2)\n",
    "\n",
    "    #X_pca = pca.fit_transform(X)\n",
    "    pca = pca.fit(rescaledX)\n",
    "    X_pca = pca.transform(rescaledX)\n",
    "    \n",
    "    show_cum_variance_vs_components(pca, n_components)\n",
    "    fig = show_cum_variance_vs_components(pca, n_components)\n",
    "\n",
    "    # py.sign_in('franec94', 'QbLNKpC0EZB0kol0aL2Z')\n",
    "    py.iplot(fig, filename='selecting-principal-components {}'.format(scaler_method))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principal_components = [pc for pc in '2,5,6,7,8,9,10'.split(',')]\n",
    "for _, pc in enumerate(principal_components):\n",
    "    n_components = int(pc)\n",
    "    \n",
    "    cum_var_exp_up_to_n_pcs = np.cumsum(pca.explained_variance_ratio_)[n_components-1]\n",
    "    print(f\"Cumulative varation explained up to {n_components} pcs = {cum_var_exp_up_to_n_pcs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_crossval(rescaledX, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_classifier_grid_search(rescaledX, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
