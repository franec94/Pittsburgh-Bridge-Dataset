{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Space Report\n",
    "\n",
    "\n",
    "<img src=\"images/polito_logo.png\" alt=\"Polito Logo\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "## Pittsburgh Bridges Data Set\n",
    "\n",
    "<img src=\"images/andy_warhol_bridge.jpg\" alt=\"Andy Warhol Bridge\" style=\"width: 200px;\"/>\n",
    "\n",
    "    Andy Warhol Bridge - Pittsburgh.\n",
    "\n",
    "Report created by Student Francesco Maria Chiarlo s253666, for A.A 2019/2020.\n",
    "\n",
    "**Abstract**:The aim of this report is to evaluate the effectiveness of distinct, different statistical learning approaches, in particular focusing on their characteristics as well as on their advantages and backwards when applied onto a relatively small dataset as the one employed within this report, that is Pittsburgh Bridgesdataset.\n",
    "\n",
    "**Key words**:Statistical Learning, Machine Learning, Bridge Design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports Section <a class=\"anchor\" id=\"imports-section\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.all_imports import *;\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for notebook repeatability\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ INPUT DATASET\n",
    "# =========================================================================== #\n",
    "dataset_path, dataset_name, column_names, TARGET_COL = get_dataset_location()\n",
    "estimators_list, estimators_names = get_estimators()\n",
    "\n",
    "dataset, feature_vs_values = load_brdiges_dataset(dataset_path, dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_2_avoid = ['ERECTED', 'LENGTH', 'LOCATION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make distinction between Target Variable and Predictors\n",
    "# --------------------------------------------------------------------------- #\n",
    "rescaledX, y, columns = prepare_data_for_train(dataset, target_col=TARGET_COL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pricipal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_table_pc_analysis(X=rescaledX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Major Pros & Cons of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Models <a class=\"anchor\" id=\"learning-models\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters to be tested for Cross-Validation Approach\n",
    "# -----------------------------------------------------\n",
    "\n",
    "# Array used for storing graphs\n",
    "plots_names = list(map(lambda xi: f\"{xi}_learning_curve.png\", estimators_names))\n",
    "pca_kernels_list = ['linear', 'poly', 'rbf', 'cosine', 'sigmoid']\n",
    "cv_list = list(range(10, 1, -1))\n",
    "\n",
    "param_grids = []\n",
    "parmas_logreg = {\n",
    "    'penalty': ('l1', 'l2', 'elastic', None),\n",
    "    'solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
    "    'fit_intercept': (True, False),\n",
    "    'tol': (1e-4, 1e-3, 1e-2),\n",
    "    'class_weight': (None, 'balanced'),\n",
    "    'C': (10.0, 1.0, .1, .01, .001, .0001),\n",
    "    # 'random_state': (0,),\n",
    "}; param_grids.append(parmas_logreg)\n",
    "\n",
    "parmas_knn_clf = {\n",
    "    'n_neighbors': (2,3,4,5,6,7,8,9,10),\n",
    "    'weights': ('uniform', 'distance'),\n",
    "    'metric': ('euclidean', 'minkowski', 'manhattan'),\n",
    "    'leaf_size': (5, 10, 15, 30),\n",
    "    'algorithm': ('ball_tree', 'kd_tree', 'brute'),\n",
    "}; param_grids.append(parmas_knn_clf)\n",
    "\n",
    "params_sgd_clf = {\n",
    "    'loss': ('log', 'modified_huber'), # ('hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron')\n",
    "    'penalty': ('l2', 'l1', 'elasticnet'),\n",
    "    'alpha': (1e-1, 1e-2, 1e-3, 1e-4),\n",
    "    'max_iter': (50, 100, 150, 200, 500, 1000, 1500, 2000, 2500),\n",
    "    'class_weight': (None, 'balanced'),\n",
    "    'learning_rate': ('optimal',),\n",
    "    'tol': (None, 1e-2, 1e-4, 1e-5, 1e-6),\n",
    "    # 'random_state': (0,),\n",
    "}; param_grids.append(params_sgd_clf)\n",
    "\n",
    "kernel_type = 'svm-rbf-kernel'\n",
    "params_svm_clf = {\n",
    "    # 'gamma': (1e-7, 1e-4, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3, 1e+5, 1e+7),\n",
    "    'gamma': (1e-5, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3, 1e+5),\n",
    "    'max_iter':(1e+2, 1e+3, 2 * 1e+3, 5 * 1e+3, 1e+4, 1.5 * 1e+3),\n",
    "    'degree': (1,2,4,8),\n",
    "    'coef0': (.001, .01, .1, 0.0, 1.0, 10.0),\n",
    "    'shrinking': (True, False),\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid',],\n",
    "    'class_weight': (None, 'balanced'),\n",
    "    'C': (1e-4, 1e-3, 1e-2, 0.1, 1.0, 10, 1e+2, 1e+3),\n",
    "    'probability': (True,),\n",
    "}; param_grids.append(params_svm_clf)\n",
    "\n",
    "parmas_tree = {\n",
    "    'splitter': ('random', 'best'),\n",
    "    'criterion':('gini', 'entropy'),\n",
    "    'max_features': (None, 'sqrt', 'log2'),\n",
    "    'max_depth': (None, 3, 5, 7, 10,),\n",
    "    'splitter': ('best', 'random',),\n",
    "    'class_weight': (None, 'balanced'),\n",
    "}; param_grids.append(parmas_tree)\n",
    "\n",
    "parmas_random_forest = {\n",
    "    'n_estimators': list(range(2, 10)),\n",
    "    'criterion':('gini', 'entropy'),\n",
    "    'bootstrap': (True, False),\n",
    "    'min_samples_leaf': list(range(1,5)),\n",
    "    'max_features': (None, 'sqrt', 'log2'),\n",
    "    'max_depth': (None, 3, 5, 7, 10,),\n",
    "    'class_weight': (None, 'balanced', 'balanced_subsample'),\n",
    "}; param_grids.append(parmas_random_forest)\n",
    "\n",
    "# Some variables to perform different tasks\n",
    "# -----------------------------------------------------\n",
    "N_CV, N_KERNEL, N_GS = 9, 5, 6;\n",
    "nrows = N_KERNEL // 2 if N_KERNEL % 2 == 0 else N_KERNEL // 2 + 1;\n",
    "ncols = 2; grid_size = [nrows, ncols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=9\n",
    "learning_curves_by_kernels(\n",
    "# learning_curves_by_components(\n",
    "    estimators_list[:], estimators_names[:],\n",
    "    rescaledX, y,\n",
    "    train_sizes=np.linspace(.1, 1.0, 10),\n",
    "    n_components=9,\n",
    "    pca_kernels_list=pca_kernels_list[0],\n",
    "    verbose=0,\n",
    "    by_pairs=True,\n",
    "    savefigs=True,\n",
    "    scoring='accuracy',\n",
    "    figs_dest=os.path.join('figures', 'learning_curve', f\"Pcs_{n_components}\"), ignore_func=True,\n",
    "    # figsize=(20,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Learning Technique | Type of Learner | Type of Learning | Classification | Regression | Ensemble Family |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| *RandomForest* | *Ensemble Method (Meta-Estimator)* | *Supervised Learning* | *Supported* | *Supported* | *Averaging Methods* |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dest = os.path.join(\"figures\", \"n_comp_9_analysis\", \"grid_search\")\n",
    "X = rescaledX\n",
    "\n",
    "df_gs, df_auc_gs = grid_search_all_by_n_components(\n",
    "    estimators_list=estimators_list[6], \\\n",
    "    param_grids=param_grids[5],\n",
    "    estimators_names=estimators_names[6], \\\n",
    "    X=X, y=y,\n",
    "    n_components=9,\n",
    "    random_state=0, show_plots=False, show_errors=False, verbose=1, plot_dest=plot_dest, debug_var=False)\n",
    "df_9, df_9_auc = df_gs, df_auc_gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results obtained running *RandomForest Classifier* against our dataset splitted into training set and test set and adopting a different kernel trick applied to *kernel-Pca* unsupervised preprocessing method we can state generally speaking that all the such a *Statistical Learning technique* leads to a sequence of results...\n",
    "\n",
    "- speaking about __Linear kernel Pca based RandomForest Classifier__, when adoping the default threshold of *.5* for classification purposes we have a model that reaches an accuracy of *%* at test time against an accuracy of *%* at train step, while the Auc score reaches a value of *%* with a Roc Curve that shows a behavior for which the model...\n",
    "\n",
    "- observing __Polynomial kernel Pca based RandomForest Estimator__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *%* at test time against an accuracy of *%* at train step, while the Auc score reaches a value of *%*...\n",
    "\n",
    "- review __Rbf kernel Pca based RandomForest Classifier__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *%* at test time against an accuracy of *%* at train step, while the Auc score reaches a value of *%*...\n",
    "\n",
    "- looking at __Cosine kernel Pca based RandomForest Classifier__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *%* at test time against an accuracy of *%* at train step, while the Auc score reaches a value of *%*.\n",
    "\n",
    "- finally, referring to __Sigmoid kernel Pca based RandomForest Model__, we can notice that such a model exploiting a default threshold of *.5* reaches an accuracy of *%* at test time against an accuracy of *%* at train step, while the Auc score reaches a value of *%*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_widget_list_df([df_gs, df_auc_gs]) #print(df_gs); print(df_auc_gs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the table dispalyed just above that shows the details about the selected values for hyper-parameters specified during grid search, in the different situations accordingly to the fixed kernel-trick for kernel Pca unsupervised method we can state that, referring to the first two columns of *Train and Test Accuracy*, we can recognize which trials lead to more overfit results such as for *Cosine, and Sigmoid Tricks* or less overfit solution such as in the case of *Linear, Polynomial, and Rbf Trick*. Speaking about the hyper-parameters, we can say what follows:\n",
    "\n",
    "- looking at __n_estimators hyper-parameter__, which refers to the number of trees in the forest, ...\n",
    "\n",
    "- reviewing __criterion parameter__, whcich represents the *function to measure the quality of a split* where supported criteria are *“gini”* for the Gini impurity and *“entropy”* for the information gain and this parameter is *tree-specific*...\n",
    "\n",
    "- speaking about __bootstrap hyper-parameter__, we know that enabling it *bootstrap samples* are used when building trees, otherwise if disabled, the whole dataset is used to build each tree...\n",
    "\n",
    "- looking at __min_samples_leaf hyper-param__, describes the minimum number of samples required to be at a leaf node...\n",
    "\n",
    "- describing __max_features hyper-param__, a hyper-param as this referes to the number of features to consider when looking for the best split, and supported choicea are *“auto”, “sqrt”, “log2”*. If “auto”, then max_features=sqrt(n_features). If “sqrt”, then max_features=sqrt(n_features) (same as “auto”). If “log2”, then max_features=log2(n_features). And, finally If None, then max_features=n_features. We notice that the search for a split does not stop until at least one valid partition of the node samples is found, even if it requires to effectively inspect more than max_features features.\n",
    "\n",
    "- looking at __max_depth hyper-param__, it reflects the maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than *min_samples_split samples*...\n",
    "\n",
    "- viewing __class_weight hyper-param__, referes to weights associated with classes and if not given, all classes are supposed to have weight one. The *“balanced”* mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)). The *“balanced_subsample”* mode is the same as “balanced” except that weights are computed based on the bootstrap sample for every tree grown.\n",
    "\n",
    "\n",
    "If we imagine to build up an *Ensemble Classifier* from the family of *Average Methods*, which state that the underlying principle leading their creation requires to build separate and single classifiers than averaging their prediction in regression context or adopting a majority vote strategy for the classification context, we can claim that amongst the purposed Knn classifier, for sure, we could employ the classifier foudn from the first three trials because of their performance metrics and also because Ensemble Methods such as Bagging Classifier, usually work fine exploiting an ensemble of independent and fine tuned classifier differently from Boosting Methods which instead are based on weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvements and Conclusions <a class=\"anchor\" id=\"Improvements-and-conclusions\"></a>\n",
    "\n",
    "Extension that we can think of to better improve the analyses we can perform on such a relative tiny dataset many include, for preprocessing phases:\n",
    "- Selecting different *Feature Extraction ant Dimensionality Reduction Techniques* other than Pca or kernel Pca such as: \n",
    "*linear discriminant analysis (LDA)*, or *canonical correlation analysis (CCA) techniques* as a pre-processing step.\n",
    "\n",
    "Extension that we can think of to better improve the analyses we can perform on such a relative tiny dataset many include, for training phases:\n",
    "\n",
    "- Selecting different *Ensemble Methods, investigating both Average based and Boosting based Statistical Learning Methods*.\n",
    "\n",
    "Extension that we can think of to better improve the analyses we can perform on such a relative tiny dataset many include, for diagnostic analyses after having performed train and test phases:\n",
    "\n",
    "- Using other measures, indicators and ghraphical plots such as the *Total Operating Characteristic (TOC)*, since also such a measure characterizes diagnostic ability while revealing more information than the ROC. In fact for each threshold, ROC reveals two ratios, TP/(TP + FN) and FP/(FP + TN). In other words, ROC reveals hits/(hits + misses) and false alarms/(false alarms + correct rejections). On the other hand, TOC shows the total information in the contingency table for each threshold. Lastly, the TOC method reveals all of the information that the ROC method provides, plus additional important information that ROC does not reveal, i.e. the size of every entry in the contingency table for each threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References section  <a class=\"anchor\" id=\"references\"></a>\n",
    "### Main References\n",
    "- Data Domain Information part:\n",
    "    - (Deck) https://en.wikipedia.org/wiki/Deck_(bridge)\n",
    "    - (Cantilever bridge) https://en.wikipedia.org/wiki/Cantilever_bridge\n",
    "    - (Arch bridge) https://en.wikipedia.org/wiki/Deck_(bridge)\n",
    "- Machine Learning part:\n",
    "    - (Theory Book) https://jakevdp.github.io/PythonDataScienceHandbook/\n",
    "    - (Feature Extraction: PCA) https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "    - (Linear Model: Logistic Regression) https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
    "    - (Neighbor-based Learning: Knn) https://scikit-learn.org/stable/modules/neighbors.html\n",
    "    - (Stochastc Learning: SGD Classifier) https://scikit-learn.org/stable/modules/sgd.html#sgd\n",
    "    - (Discriminative Model: SVM) https://scikit-learn.org/stable/modules/svm.html\n",
    "    - (Non-Parametric Learning: Decsion Trees) https://scikit-learn.org/stable/modules/tree.html#tree\n",
    "    - (Ensemble, Non-Parametric Learning: RandomForest) https://scikit-learn.org/stable/modules/ensemble.html#forest\n",
    "- Metrics:\n",
    "    - (F1-Accuracy-Precision-Recall) https://towardsdatascience.com/beyond-accuracy-precision-and-recall-3da06bea9f6c\n",
    "- Statistics:\n",
    "    - (Correlation and dependence) https://en.wikipedia.org/wiki/Correlation_and_dependence\n",
    "    - (KDE) https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/\n",
    "- Chart part:\n",
    "    - (Seaborn Charts) https://acadgild.com/blog/data-visualization-using-matplotlib-and-seaborn\n",
    "- Third Party Library:\n",
    "    - (sklearn) https://scikit-learn.org/stable/index.html\n",
    "    - (statsmodels) https://www.statsmodels.org/stable/index.html#\n",
    "        \n",
    "### Others References\n",
    "- Plots:\n",
    "    - (Python Plot) https://www.datacamp.com/community/tutorials/matplotlib-tutorial-python?utm_source=adwords_ppc&utm_campaignid=898687156&utm_adgroupid=48947256715&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=255798340456&utm_targetid=aud-299261629574:dsa-473406587955&utm_loc_interest_ms=&utm_loc_physical_ms=1008025&gclid=Cj0KCQjw-_j1BRDkARIsAJcfmTFu4LAUDhRGK2D027PHiqIPSlxK3ud87Ek_lwOu8rt8A8YLrjFiHqsaAoLDEALw_wcB\n",
    "- Markdown Math part:\n",
    "    - (Math Symbols Latex) https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols\n",
    "    - (Tutorial 1) https://share.cocalc.com/share/b4a30ed038ee41d868dad094193ac462ccd228e2/Homework%20/HW%201.2%20-%20Markdown%20and%20LaTeX%20Cheatsheet.ipynb?viewer=share\n",
    "    - (Tutorial 2) https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Typesetting%20Equations.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
